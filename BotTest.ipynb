{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be31421a",
   "metadata": {},
   "source": [
    "## Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG (non-streaming only)\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_BATCH = f\"{API_BASE}/currency/exchange-rates/batch\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD for selecting folders (folders are YYYY-MM-DD)\n",
    "START_DATE = \"07-07-2025\"\n",
    "END_DATE   = \"15-07-2025\"\n",
    "\n",
    "# Batch size per request (set to 20 by default; change as needed)\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 30000  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # if True, stop immediately when a batch fails\n",
    "\n",
    "# Input hygiene\n",
    "DROP_SAME_CURRENCY = True  # drop items where FromCurrency == ToCurrency\n",
    "\n",
    "# Output\n",
    "WRITE_DAY_SUMMARY = True  # write per-day JSON summary under each day folder\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _load_payload_for_day(day_dir: str) -> Optional[List[Any]]:\n",
    "    \"\"\"Read exchange_rates_payload.json and return AS-IS (no normalization).\"\"\"\n",
    "    path = os.path.join(day_dir, \"exchange_rates_payload.json\")\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[skip] payload not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            print(f\"[warn] payload is not a list, skipping: {path}\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] failed reading payload {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _chunked(lst: List[Any], size: int) -> List[List[Any]]:\n",
    "    size = max(1, int(size))\n",
    "    return [lst[i:i+size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def _dedupe_payload(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Deduplicate by logical key; fallback to full JSON. Preserve order.\"\"\"\n",
    "    seen = set()\n",
    "    out: List[Any] = []\n",
    "    removed = 0\n",
    "    for item in payload:\n",
    "        key: Optional[Tuple[Any, ...]] = None\n",
    "        if isinstance(item, dict):\n",
    "            fields = (\n",
    "                item.get(\"ExchangeRateType\"),\n",
    "                item.get(\"FromCurrency\"),\n",
    "                item.get(\"ToCurrency\"),\n",
    "                item.get(\"ValidFrom\"),\n",
    "                item.get(\"Quotation\"),\n",
    "            )\n",
    "            if all(v is not None for v in fields):\n",
    "                key = (\"LOGIC_KEY\",) + fields\n",
    "        if key is None:\n",
    "            try:\n",
    "                canonical = json.dumps(item, sort_keys=True, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                canonical = repr(item)\n",
    "            key = (\"RAW_ITEM\", canonical)\n",
    "        if key in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(item)\n",
    "    return out, removed\n",
    "\n",
    "def _filter_same_currency(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Drop rows where FromCurrency == ToCurrency (exact string compare).\"\"\"\n",
    "    out: List[Any] = []\n",
    "    dropped = 0\n",
    "    for it in payload:\n",
    "        if isinstance(it, dict):\n",
    "            f = it.get(\"FromCurrency\")\n",
    "            t = it.get(\"ToCurrency\")\n",
    "            if f is not None and t is not None and str(f) == str(t):\n",
    "                dropped += 1\n",
    "                continue\n",
    "        out.append(it)\n",
    "    return out, dropped\n",
    "\n",
    "def _post_with_retries(url: str, *, json_body: Any, timeout: Optional[int]) -> requests.Response:\n",
    "    \"\"\"POST with exponential backoff on 429/5xx/connect/read issues.\"\"\"\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, json=json_body, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_payload_batch(payload: List[Any]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"Always post to the non-streaming /batch endpoint.\"\"\"\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_BATCH, json_body=payload, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body = e.response.text\n",
    "            except Exception:\n",
    "                body = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body) if body else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, *, total_rows: int, rows_after_filters: int,\n",
    "                       dupes_removed: int, same_currency_dropped: int,\n",
    "                       posted_batches: int, rows_sent: int, errors: int) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"total_rows_in_file\": total_rows,\n",
    "            \"rows_after_filters\": rows_after_filters,\n",
    "            \"duplicates_removed\": dupes_removed,\n",
    "            \"same_currency_dropped\": same_currency_dropped,\n",
    "            \"posted_batches\": posted_batches,\n",
    "            \"rows_sent\": rows_sent,\n",
    "            \"errors\": errors,\n",
    "            \"endpoint\": \"batch\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(day_dir, \"post_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN (strict day->batches sequence)\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    bs = max(1, int(BATCH_SIZE))\n",
    "\n",
    "    total_days = 0\n",
    "    days_with_payload = 0\n",
    "    posted_days = 0\n",
    "    posted_batches = 0\n",
    "    rows_sent = 0\n",
    "    skipped_days = 0\n",
    "    errors = 0\n",
    "    total_dupes_removed = 0\n",
    "    total_same_currency_dropped = 0\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] batch_size = {bs}, endpoint = /batch\")\n",
    "    print(\"[mode] STRICT ORDER: per-day, then per-day batches (no interleaving)\\n\")\n",
    "\n",
    "    # STRICT: iterate days in order\n",
    "    for d in _daterange_inclusive(start_dt, end_dt):\n",
    "        total_days += 1\n",
    "        day_name = d.strftime(\"%Y-%m-%d\")\n",
    "        day_dir = os.path.join(BASE_DIR, day_name)\n",
    "        print(f\"\\n=== DAY {day_name} ===\")\n",
    "\n",
    "        if not os.path.isdir(day_dir):\n",
    "            print(f\"[skip] day folder missing: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        payload = _load_payload_for_day(day_dir)\n",
    "        if not payload:\n",
    "            print(f\"[skip] no valid payload in: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        days_with_payload += 1\n",
    "        total_rows_in_file = len(payload)\n",
    "\n",
    "        # Hygiene for the day (still AS-IS structure)\n",
    "        deduped_payload, dupes_removed = _dedupe_payload(payload)\n",
    "        total_dupes_removed += dupes_removed\n",
    "\n",
    "        if DROP_SAME_CURRENCY:\n",
    "            filtered_payload, same_drop = _filter_same_currency(deduped_payload)\n",
    "        else:\n",
    "            filtered_payload, same_drop = deduped_payload, 0\n",
    "        total_same_currency_dropped += same_drop\n",
    "\n",
    "        n_after = len(filtered_payload)\n",
    "        print(f\"[day] rows: {total_rows_in_file} → {n_after} \"\n",
    "              f\"(dedup removed {dupes_removed}, same-currency dropped {same_drop})\")\n",
    "\n",
    "        if n_after == 0:\n",
    "            print(f\"[day] {day_name}: nothing to post after filters; skipping\")\n",
    "            _write_day_summary(day_dir,\n",
    "                               total_rows=total_rows_in_file,\n",
    "                               rows_after_filters=n_after,\n",
    "                               dupes_removed=dupes_removed,\n",
    "                               same_currency_dropped=same_drop,\n",
    "                               posted_batches=0,\n",
    "                               rows_sent=0,\n",
    "                               errors=0)\n",
    "            continue\n",
    "\n",
    "        # STRICT: process this day's batches sequentially\n",
    "        chunks = _chunked(filtered_payload, bs)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"[day] batching: {total_chunks} batch(es) of up to {bs}\")\n",
    "\n",
    "        day_had_success = False\n",
    "        day_errors = 0\n",
    "        day_rows_sent = 0\n",
    "        day_batches_posted = 0\n",
    "\n",
    "        for idx, batch in enumerate(chunks, start=1):\n",
    "            print(f\"[post] {day_name} | batch {idx}/{total_chunks}: {len(batch)} rows → /batch\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_payload_batch(batch)\n",
    "\n",
    "            if ok:\n",
    "                day_had_success = True\n",
    "                day_batches_posted += 1\n",
    "                posted_batches += 1\n",
    "                day_rows_sent += len(batch)\n",
    "                rows_sent += len(batch)\n",
    "\n",
    "                if resp_json is not None:\n",
    "                    # Uncomment for verbose:\n",
    "                    # print(json.dumps(resp_json, indent=2, ensure_ascii=False)[:2000])\n",
    "                    pass\n",
    "                elif resp_text is not None:\n",
    "                    print(resp_text[:1000])\n",
    "                else:\n",
    "                    print(\"[info] posted OK (no response body)\")\n",
    "            else:\n",
    "                day_errors += 1\n",
    "                errors += 1\n",
    "                print(f\"[error] POST failed for {day_name} batch {idx}/{total_chunks}:\\n\"\n",
    "                      f\"{(resp_text or '(no body)')[:1000]}\")\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting at this batch\")\n",
    "                    # write partial day summary then stop everything\n",
    "                    _write_day_summary(day_dir,\n",
    "                                       total_rows=total_rows_in_file,\n",
    "                                       rows_after_filters=n_after,\n",
    "                                       dupes_removed=dupes_removed,\n",
    "                                       same_currency_dropped=same_drop,\n",
    "                                       posted_batches=day_batches_posted,\n",
    "                                       rows_sent=day_rows_sent,\n",
    "                                       errors=day_errors)\n",
    "                    print(\"\\n[summary]\")\n",
    "                    print(f\"  days in range         : {total_days}\")\n",
    "                    print(f\"  days with payload     : {days_with_payload}\")\n",
    "                    print(f\"  posted days           : {posted_days}\")\n",
    "                    print(f\"  posted batches        : {posted_batches}\")\n",
    "                    print(f\"  rows sent             : {rows_sent}\")\n",
    "                    print(f\"  skipped days          : {skipped_days}\")\n",
    "                    print(f\"  errors                : {errors}\")\n",
    "                    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "                    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "                    return\n",
    "\n",
    "        if day_had_success:\n",
    "            posted_days += 1\n",
    "\n",
    "        _write_day_summary(day_dir,\n",
    "                           total_rows=total_rows_in_file,\n",
    "                           rows_after_filters=n_after,\n",
    "                           dupes_removed=dupes_removed,\n",
    "                           same_currency_dropped=same_drop,\n",
    "                           posted_batches=day_batches_posted,\n",
    "                           rows_sent=day_rows_sent,\n",
    "                           errors=day_errors)\n",
    "\n",
    "    # Final overall summary\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  days in range         : {total_days}\")\n",
    "    print(f\"  days with payload     : {days_with_payload}\")\n",
    "    print(f\"  posted days           : {posted_days}\")\n",
    "    print(f\"  posted batches        : {posted_batches}\")\n",
    "    print(f\"  rows sent             : {rows_sent}\")\n",
    "    print(f\"  skipped days          : {skipped_days}\")\n",
    "    print(f\"  errors                : {errors}\")\n",
    "    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b2007",
   "metadata": {},
   "source": [
    "# Draft Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe6f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-08-18 → 2025-08-18 (inclusive)\n",
      "[config] endpoint=/currency/exchange-rates/drafts/delete, per_day=True, max_per_day=None\n",
      "\n",
      "\n",
      "=== DAY 2025-08-18 ===\n",
      "[day] deleted: 12\n",
      "\n",
      "[summary]\n",
      "  days in range   : 1\n",
      "  processed days  : 1\n",
      "  total deleted   : 12\n",
      "  skipped days    : 0\n",
      "  errors          : 0\n"
     ]
    }
   ],
   "source": [
    "# post_delete_drafts.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_DELETE = f\"{API_BASE}/currency/exchange-rates/drafts/delete\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD\n",
    "START_DATE = \"18-08-2025\"\n",
    "END_DATE   = \"18-08-2025\"\n",
    "\n",
    "# If True → call the API one day at a time (date_from=day=date_to).\n",
    "# If False → call the API once with the full inclusive range.\n",
    "PER_DAY = True\n",
    "\n",
    "# Optional cap on deletions per day. Set to None to remove the limit.\n",
    "MAX_PER_DAY: Optional[int] = None  # e.g., 200\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 300  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # if True, halt immediately on a failed POST\n",
    "\n",
    "# Output\n",
    "WRITE_DAY_SUMMARY = True   # write per-day JSON summary under each day folder\n",
    "WRITE_RANGE_SUMMARY = True # write a summary json under BASE_DIR when PER_DAY=False\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _post_with_retries(url: str, *, params: Dict[str, Any], timeout: Optional[int]) -> requests.Response:\n",
    "    \"\"\"POST with exponential backoff on 429/5xx/connect/read issues.\"\"\"\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, params=params, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_delete(day_from: str, day_to: str, max_per_day: Optional[int]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Calls /currency/exchange-rates/drafts/delete using query params:\n",
    "      ?date_from=YYYY-MM-DD&date_to=YYYY-MM-DD&max_per_day=INT\n",
    "    If date_from == date_to, server processes one day only (as required).\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\"date_from\": day_from, \"date_to\": day_to}\n",
    "    if max_per_day is not None:\n",
    "        params[\"max_per_day\"] = int(max_per_day)\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_DELETE, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body_txt = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body_txt = e.response.text\n",
    "            except Exception:\n",
    "                body_txt = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body_txt) if body_txt else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, *, deleted_count: int, deleted_sample: Optional[list], ok: bool, why: str = \"\") -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"deleted_count\": int(deleted_count),\n",
    "            \"deleted_sample\": deleted_sample or [],\n",
    "            \"ok\": bool(ok),\n",
    "            \"why\": why,\n",
    "            \"endpoint\": \"drafts/delete\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        _ensure_dir(day_dir)\n",
    "        p = os.path.join(day_dir, \"delete_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "def _write_range_summary(base_dir: str, *, start_date: str, end_date: str, total_deleted: int, days_processed: int, errors: int) -> None:\n",
    "    if not WRITE_RANGE_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"range\": {\"from\": start_date, \"to\": end_date},\n",
    "            \"total_deleted\": int(total_deleted),\n",
    "            \"days_processed\": int(days_processed),\n",
    "            \"errors\": int(errors),\n",
    "            \"endpoint\": \"drafts/delete\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(base_dir, \"delete_summary_range.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write range summary failed: {e}\")\n",
    "\n",
    "def _iso(d: datetime) -> str:\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] endpoint=/currency/exchange-rates/drafts/delete, per_day={PER_DAY}, max_per_day={MAX_PER_DAY}\\n\")\n",
    "\n",
    "    if PER_DAY:\n",
    "        # ---------- one API call per day ----------\n",
    "        total_days = 0\n",
    "        days_processed = 0\n",
    "        total_deleted = 0\n",
    "        skipped_days = 0\n",
    "        errors = 0\n",
    "\n",
    "        for d in _daterange_inclusive(start_dt, end_dt):\n",
    "            total_days += 1\n",
    "            day_iso = _iso(d)\n",
    "            day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "            print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_delete(day_iso, day_iso, MAX_PER_DAY)\n",
    "            if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "                days_processed += 1\n",
    "                deleted_count = int(resp_json.get(\"total_deleted\", 0))\n",
    "                total_deleted += deleted_count\n",
    "                deleted_sample = []\n",
    "                # Try to collect a small sample from 'per_day' entry in server payload\n",
    "                try:\n",
    "                    per_day = resp_json.get(\"per_day\") or []\n",
    "                    if isinstance(per_day, list):\n",
    "                        for entry in per_day:\n",
    "                            if isinstance(entry, dict) and entry.get(\"date\") == day_iso:\n",
    "                                deleted_sample = entry.get(\"sample\", []) or []\n",
    "                                break\n",
    "                except Exception:\n",
    "                    deleted_sample = []\n",
    "\n",
    "                print(f\"[day] deleted: {deleted_count}\")\n",
    "                _write_day_summary(day_dir,\n",
    "                                deleted_count=deleted_count,\n",
    "                                deleted_sample=deleted_sample[:10],\n",
    "                                ok=True)\n",
    "            else:\n",
    "                errors += 1\n",
    "                why = \"\"\n",
    "                if resp_json and isinstance(resp_json, dict):\n",
    "                    why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "                elif resp_text:\n",
    "                    why = str(resp_text)[:400]\n",
    "                print(f\"[error] delete failed for {day_iso}: {why or '(no details)'}\")\n",
    "                _write_day_summary(day_dir, deleted_count=0, deleted_sample=[], ok=False, why=why)\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "                    _write_range_summary(BASE_DIR,\n",
    "                                         start_date=_iso(start_dt),\n",
    "                                         end_date=_iso(end_dt),\n",
    "                                         total_deleted=total_deleted,\n",
    "                                         days_processed=days_processed,\n",
    "                                         errors=errors)\n",
    "                    return\n",
    "                skipped_days += 1\n",
    "\n",
    "        # Final summary\n",
    "        print(\"\\n[summary]\")\n",
    "        print(f\"  days in range   : {total_days}\")\n",
    "        print(f\"  processed days  : {days_processed}\")\n",
    "        print(f\"  total deleted   : {total_deleted}\")\n",
    "        print(f\"  skipped days    : {skipped_days}\")\n",
    "        print(f\"  errors          : {errors}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=_iso(start_dt),\n",
    "                             end_date=_iso(end_dt),\n",
    "                             total_deleted=total_deleted,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "    else:\n",
    "        # ---------- one API call for the entire range ----------\n",
    "        df = _iso(start_dt)\n",
    "        dt = _iso(end_dt)\n",
    "        print(f\"[range] single call: {df}..{dt}\")\n",
    "        ok, resp_json, resp_text = _post_delete(df, dt, MAX_PER_DAY)\n",
    "\n",
    "        total_deleted = 0\n",
    "        days_processed = 0\n",
    "        errors = 0\n",
    "\n",
    "        if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "            total_deleted = int(resp_json.get(\"total_deleted\", 0))\n",
    "            days_processed = int(resp_json.get(\"days_processed\", 0))\n",
    "            print(f\"[range] deleted={total_deleted}, days_processed={days_processed}\")\n",
    "        else:\n",
    "            errors = 1\n",
    "            why = \"\"\n",
    "            if resp_json and isinstance(resp_json, dict):\n",
    "                why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "            elif resp_text:\n",
    "                why = str(resp_text)[:400]\n",
    "            print(f\"[error] range delete failed: {why or '(no details)'}\")\n",
    "            if STOP_ON_ERROR:\n",
    "                print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=df,\n",
    "                             end_date=dt,\n",
    "                             total_deleted=total_deleted,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e20ba",
   "metadata": {},
   "source": [
    "# Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6c3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-07-10 → 2025-07-31 (inclusive)\n",
      "[config] endpoint=/currency/exchange-rates/fallback/collect-missing, per_day=True\n",
      "\n",
      "\n",
      "=== DAY 2025-07-10 ===\n",
      "[day] excel_rows=383 json_rows=386 missing=3\n",
      "\n",
      "=== DAY 2025-07-11 ===\n",
      "[day] excel_rows=379 json_rows=386 missing=7\n",
      "\n",
      "=== DAY 2025-07-12 ===\n",
      "[day] excel_rows=382 json_rows=386 missing=4\n",
      "\n",
      "=== DAY 2025-07-13 ===\n",
      "[day] excel_rows=380 json_rows=386 missing=6\n",
      "\n",
      "=== DAY 2025-07-14 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-15 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-16 ===\n",
      "[day] excel_rows=385 json_rows=386 missing=1\n",
      "\n",
      "=== DAY 2025-07-17 ===\n",
      "[day] excel_rows=383 json_rows=386 missing=3\n",
      "\n",
      "=== DAY 2025-07-18 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-19 ===\n",
      "[day] excel_rows=383 json_rows=386 missing=3\n",
      "\n",
      "=== DAY 2025-07-20 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-21 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-22 ===\n",
      "[day] excel_rows=376 json_rows=386 missing=10\n",
      "\n",
      "=== DAY 2025-07-23 ===\n",
      "[day] excel_rows=374 json_rows=386 missing=12\n",
      "\n",
      "=== DAY 2025-07-24 ===\n",
      "[day] excel_rows=374 json_rows=386 missing=386\n",
      "\n",
      "=== DAY 2025-07-25 ===\n",
      "[day] excel_rows=382 json_rows=386 missing=4\n",
      "\n",
      "=== DAY 2025-07-26 ===\n",
      "[day] excel_rows=383 json_rows=386 missing=3\n",
      "\n",
      "=== DAY 2025-07-27 ===\n",
      "[day] excel_rows=378 json_rows=386 missing=8\n",
      "\n",
      "=== DAY 2025-07-28 ===\n",
      "[day] excel_rows=378 json_rows=386 missing=8\n",
      "\n",
      "=== DAY 2025-07-29 ===\n",
      "[day] excel_rows=381 json_rows=386 missing=5\n",
      "\n",
      "=== DAY 2025-07-30 ===\n",
      "[day] excel_rows=379 json_rows=386 missing=7\n",
      "\n",
      "=== DAY 2025-07-31 ===\n",
      "[day] excel_rows=380 json_rows=386 missing=6\n",
      "\n",
      "[summary]\n",
      "  processed days  : 22\n",
      "  total missing   : 501\n",
      "  errors          : 0\n"
     ]
    }
   ],
   "source": [
    "# post_fallback_collect_missing.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL = f\"{API_BASE}/currency/exchange-rates/fallback/collect-missing\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD\n",
    "START_DATE = \"10-07-2025\"\n",
    "END_DATE   = \"31-07-2025\"\n",
    "\n",
    "PER_DAY = True\n",
    "REQUEST_TIMEOUT = 300\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2\n",
    "STOP_ON_ERROR = False\n",
    "WRITE_DAY_SUMMARY = True\n",
    "WRITE_RANGE_SUMMARY = True\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _post_with_retries(url: str, *, params: Dict[str, Any], timeout: Optional[int]) -> requests.Response:\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, params=params, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_collect(day_from: str, day_to: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    params: Dict[str, Any] = {\"date_from\": day_from, \"date_to\": day_to}\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body_txt = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body_txt = e.response.text\n",
    "            except Exception:\n",
    "                body_txt = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body_txt) if body_txt else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, payload: Dict[str, Any]) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        _ensure_dir(day_dir)\n",
    "        p = os.path.join(day_dir, \"fallback_collect_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "def _write_range_summary(base_dir: str, *, start_date: str, end_date: str, total_missing: int, days_processed: int, errors: int) -> None:\n",
    "    if not WRITE_RANGE_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"range\": {\"from\": start_date, \"to\": end_date},\n",
    "            \"total_missing\": int(total_missing),\n",
    "            \"days_processed\": int(days_processed),\n",
    "            \"errors\": int(errors),\n",
    "            \"endpoint\": \"fallback/collect-missing\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(base_dir, \"fallback_collect_summary_range.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write range summary failed: {e}\")\n",
    "\n",
    "def _iso(d: datetime) -> str:\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] endpoint=/currency/exchange-rates/fallback/collect-missing, per_day={PER_DAY}\\n\")\n",
    "\n",
    "    total_missing = 0\n",
    "    days_processed = 0\n",
    "    errors = 0\n",
    "\n",
    "    if PER_DAY:\n",
    "        for d in _daterange_inclusive(start_dt, end_dt):\n",
    "            day_iso = _iso(d)\n",
    "            day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "            print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_collect(day_iso, day_iso)\n",
    "            if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "                # grab the day row (if present)\n",
    "                per_day = resp_json.get(\"per_day\") or []\n",
    "                found = None\n",
    "                for entry in per_day:\n",
    "                    if isinstance(entry, dict) and entry.get(\"date\") == day_iso:\n",
    "                        found = entry\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    days_processed += 1\n",
    "                    total_missing += int(found.get(\"missing\", 0))\n",
    "                    print(f\"[day] excel_rows={found.get('excel_rows')} json_rows={found.get('json_rows')} missing={found.get('missing')}\")\n",
    "                    _write_day_summary(day_dir, {\n",
    "                        \"missing_count\": found.get(\"missing\", 0),\n",
    "                        \"excel_rows\": found.get(\"excel_rows\", 0),\n",
    "                        \"json_rows\": found.get(\"json_rows\", 0),\n",
    "                        \"tracker_path\": found.get(\"tracker_path\", \"\"),\n",
    "                        \"export_clicked\": found.get(\"export_clicked\", False),\n",
    "                        \"xlsx_path\": found.get(\"xlsx_path\", \"\"),\n",
    "                        \"xlsx_size\": found.get(\"xlsx_size\", 0),\n",
    "                        \"headers_seen\": found.get(\"headers_seen\", []),\n",
    "                        \"json_file_exists\": found.get(\"json_file_exists\", False),\n",
    "                        \"ok\": True,\n",
    "                        \"why\": \"\",\n",
    "                        \"endpoint\": \"fallback/collect-missing\",\n",
    "                        \"ts\": datetime.now().isoformat(),\n",
    "                    })\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    print(f\"[warn] API ok but 'per_day' did not contain {day_iso}\")\n",
    "                    _write_day_summary(day_dir, {\n",
    "                        \"missing_count\": 0, \"excel_rows\": 0, \"json_rows\": 0,\n",
    "                        \"tracker_path\": \"\", \"export_clicked\": False, \"xlsx_path\": \"\",\n",
    "                        \"xlsx_size\": 0, \"headers_seen\": [], \"json_file_exists\": False,\n",
    "                        \"ok\": False, \"why\": \"no_day_entry_in_response\",\n",
    "                        \"endpoint\": \"fallback/collect-missing\", \"ts\": datetime.now().isoformat(),\n",
    "                    })\n",
    "\n",
    "            else:\n",
    "                errors += 1\n",
    "                why = \"\"\n",
    "                if resp_json and isinstance(resp_json, dict):\n",
    "                    why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "                elif resp_text:\n",
    "                    why = str(resp_text)[:400]\n",
    "                print(f\"[error] collect failed for {day_iso}: {why or '(no details)'}\")\n",
    "                _write_day_summary(day_dir, {\n",
    "                    \"missing_count\": 0, \"excel_rows\": 0, \"json_rows\": 0,\n",
    "                    \"tracker_path\": \"\", \"export_clicked\": False, \"xlsx_path\": \"\",\n",
    "                    \"xlsx_size\": 0, \"headers_seen\": [], \"json_file_exists\": False,\n",
    "                    \"ok\": False, \"why\": why,\n",
    "                    \"endpoint\": \"fallback/collect-missing\", \"ts\": datetime.now().isoformat(),\n",
    "                })\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "                    _write_range_summary(BASE_DIR,\n",
    "                                         start_date=_iso(start_dt),\n",
    "                                         end_date=_iso(end_dt),\n",
    "                                         total_missing=total_missing,\n",
    "                                         days_processed=days_processed,\n",
    "                                         errors=errors)\n",
    "                    return\n",
    "\n",
    "        # summary\n",
    "        print(\"\\n[summary]\")\n",
    "        print(f\"  processed days  : {days_processed}\")\n",
    "        print(f\"  total missing   : {total_missing}\")\n",
    "        print(f\"  errors          : {errors}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=_iso(start_dt),\n",
    "                             end_date=_iso(end_dt),\n",
    "                             total_missing=total_missing,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "    else:\n",
    "        # Range call (server already iterates day-by-day)\n",
    "        df = _iso(start_dt)\n",
    "        dt = _iso(end_dt)\n",
    "        print(f\"[range] single call: {df}..{dt}\")\n",
    "        ok, resp_json, resp_text = _post_collect(df, dt)\n",
    "\n",
    "        if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "            per_day = resp_json.get(\"per_day\") or []\n",
    "            days_processed = len([x for x in per_day if x.get(\"ok\")])\n",
    "            total_missing = sum(int(x.get(\"missing\", 0)) for x in per_day)\n",
    "            for found in per_day:\n",
    "                day_iso = found.get(\"date\") or \"\"\n",
    "                if not day_iso:\n",
    "                    continue\n",
    "                day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "                _write_day_summary(day_dir, {\n",
    "                    \"missing_count\": found.get(\"missing\", 0),\n",
    "                    \"excel_rows\": found.get(\"excel_rows\", 0),\n",
    "                    \"json_rows\": found.get(\"json_rows\", 0),\n",
    "                    \"tracker_path\": found.get(\"tracker_path\", \"\"),\n",
    "                    \"export_clicked\": found.get(\"export_clicked\", False),\n",
    "                    \"xlsx_path\": found.get(\"xlsx_path\", \"\"),\n",
    "                    \"xlsx_size\": found.get(\"xlsx_size\", 0),\n",
    "                    \"headers_seen\": found.get(\"headers_seen\", []),\n",
    "                    \"json_file_exists\": found.get(\"json_file_exists\", False),\n",
    "                    \"ok\": found.get(\"ok\", False),\n",
    "                    \"why\": found.get(\"why\", \"\"),\n",
    "                    \"endpoint\": \"fallback/collect-missing\",\n",
    "                    \"ts\": datetime.now().isoformat(),\n",
    "                })\n",
    "        else:\n",
    "            errors = 1\n",
    "            why = \"\"\n",
    "            if resp_json and isinstance(resp_json, dict):\n",
    "                why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "            elif resp_text:\n",
    "                why = str(resp_text)[:400]\n",
    "            print(f\"[error] range collect failed: {why or '(no details)'}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=df,\n",
    "                             end_date=dt,\n",
    "                             total_missing=total_missing,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeaad35",
   "metadata": {},
   "source": [
    "# post_fallback_refill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaba4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-07-25 → 2025-07-31 (inclusive)\n",
      "[config] batch_size = 50, source = WebService/TrackDrivers/Fallback, endpoint = /batch\n",
      "\n",
      "\n",
      "=== DAY 2025-07-25 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 50\n",
      "[post] 2025-07-25 | batch 1/1: 4 rows\n",
      "\n",
      "=== DAY 2025-07-26 ===\n",
      "[day] rows: 3 → 3 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 50\n",
      "[post] 2025-07-26 | batch 1/1: 3 rows\n"
     ]
    }
   ],
   "source": [
    "# post_fallback_refill.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_BATCH = f\"{API_BASE}/currency/exchange-rates/batch\"  # non-streaming\n",
    "\n",
    "# Where the fallback collector saved the “missing” rows\n",
    "FALLBACK_DIR = \"WebService/TrackDrivers/Fallback\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD (files are named YYYY-MM-DD.json)\n",
    "START_DATE = \"25-07-2025\"\n",
    "END_DATE   = \"31-07-2025\"\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 300  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential backoff base)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # stop immediately if a batch fails\n",
    "\n",
    "# Input hygiene\n",
    "DROP_SAME_CURRENCY = True  # drop items where FromCurrency == ToCurrency\n",
    "ROUND_RATE_5DP = True      # coerce ExchangeRate to string with 5 dp (API model expectation)\n",
    "\n",
    "# Output (writes next to each <YYYY-MM-DD>.json in FALLBACK_DIR)\n",
    "WRITE_DAY_SUMMARY = True\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _chunked(lst: List[Any], size: int) -> List[List[Any]]:\n",
    "    size = max(1, int(size))\n",
    "    return [lst[i:i+size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def _post_with_retries(url: str, *, json_body: Any, timeout: Optional[int]) -> requests.Response:\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, json=json_body, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_payload_batch(payload: List[Dict[str, Any]]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_BATCH, json_body=payload, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body = e.response.text\n",
    "            except Exception:\n",
    "                body = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body) if body else ''}\"\n",
    "\n",
    "# ---------- normalization used by API model (ExchangeRateItem) ----------\n",
    "def _q_norm(q: str | None) -> str:\n",
    "    s = (q or \"Direct\").strip().lower()\n",
    "    return \"Indirect\" if s.startswith(\"ind\") else \"Direct\"\n",
    "\n",
    "def _date_to_ddmmyyyy(s: str | None) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    fmts = [\n",
    "        \"%d.%m.%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\",\n",
    "        \"%Y/%m/%d\", \"%Y%m%d\", \"%Y-%d-%m\"\n",
    "    ]\n",
    "    for f in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, f)\n",
    "            return dt.strftime(\"%d.%m.%Y\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # If it already looks like DD.MM.YYYY but failed parse for some reason, keep as-is\n",
    "    return s\n",
    "\n",
    "def _rate_5dp(v: Any) -> str:\n",
    "    q = Decimal(str(v))\n",
    "    if q <= 0:\n",
    "        # Let API reject if invalid; still format\n",
    "        q = Decimal(\"0.00001\")\n",
    "    q = q.quantize(Decimal(\"0.00001\"), rounding=ROUND_HALF_UP)\n",
    "    return f\"{q:.5f}\"\n",
    "\n",
    "def _norm_row(r: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        return {\n",
    "            \"ExchangeRateType\": (r.get(\"ExchangeRateType\") or \"\").strip().upper(),\n",
    "            \"FromCurrency\": (r.get(\"FromCurrency\") or \"\").strip().upper(),\n",
    "            \"ToCurrency\": (r.get(\"ToCurrency\") or \"\").strip().upper(),\n",
    "            \"ValidFrom\": _date_to_ddmmyyyy(r.get(\"ValidFrom\")),\n",
    "            \"Quotation\": _q_norm(r.get(\"Quotation\")),\n",
    "            \"ExchangeRate\": _rate_5dp(r.get(\"ExchangeRate\")),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _dedupe_payload(payload: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:\n",
    "    \"\"\"\n",
    "    Deduplicate by logical key (Type, From, To, Date, Quotation). Preserve order.\n",
    "    \"\"\"\n",
    "    seen: set[Tuple[str, str, str, str, str]] = set()\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    removed = 0\n",
    "    for r in payload:\n",
    "        key = (\n",
    "            (r.get(\"ExchangeRateType\") or \"\").upper(),\n",
    "            (r.get(\"FromCurrency\") or \"\").upper(),\n",
    "            (r.get(\"ToCurrency\") or \"\").upper(),\n",
    "            (r.get(\"ValidFrom\") or \"\"),\n",
    "            _q_norm(r.get(\"Quotation\")),\n",
    "        )\n",
    "        if key in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(r)\n",
    "    return out, removed\n",
    "\n",
    "def _filter_same_currency(payload: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    dropped = 0\n",
    "    for r in payload:\n",
    "        f = (r.get(\"FromCurrency\") or \"\").upper()\n",
    "        t = (r.get(\"ToCurrency\") or \"\").upper()\n",
    "        if f and t and f == t and DROP_SAME_CURRENCY:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        out.append(r)\n",
    "    return out, dropped\n",
    "\n",
    "# ---------- IO ----------\n",
    "def _load_fallback_for_day(day_iso: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Read WebService/TrackDrivers/Fallback/<YYYY-MM-DD>.json\n",
    "    Return list[dict] or None if missing/bad.\n",
    "    \"\"\"\n",
    "    path = os.path.join(FALLBACK_DIR, f\"{day_iso}.json\")\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[skip] fallback file not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f) or []\n",
    "        if not isinstance(data, list):\n",
    "            print(f\"[warn] fallback file is not a list: {path}\")\n",
    "            return None\n",
    "        # Normalize rows to API model shape\n",
    "        norm: List[Dict[str, Any]] = []\n",
    "        for r in data:\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            nr = _norm_row(r)\n",
    "            if nr:\n",
    "                norm.append(nr)\n",
    "        return norm\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] failed reading fallback {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _write_day_summary(day_iso: str, *, total_rows: int, rows_after_filters: int,\n",
    "                       dupes_removed: int, same_currency_dropped: int,\n",
    "                       posted_batches: int, rows_sent: int, errors: int) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"total_rows_in_fallback\": total_rows,\n",
    "            \"rows_after_filters\": rows_after_filters,\n",
    "            \"duplicates_removed\": dupes_removed,\n",
    "            \"same_currency_dropped\": same_currency_dropped,\n",
    "            \"posted_batches\": posted_batches,\n",
    "            \"rows_sent\": rows_sent,\n",
    "            \"errors\": errors,\n",
    "            \"endpoint\": \"/currency/exchange-rates/batch\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        path = os.path.join(FALLBACK_DIR, f\"{day_iso}.refill_summary.json\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write summary failed for {day_iso}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    bs = max(1, int(BATCH_SIZE))\n",
    "\n",
    "    total_days = 0\n",
    "    posted_days = 0\n",
    "    posted_batches = 0\n",
    "    rows_sent = 0\n",
    "    skipped_days = 0\n",
    "    errors = 0\n",
    "    total_dupes_removed = 0\n",
    "    total_same_currency_dropped = 0\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] batch_size = {bs}, source = {FALLBACK_DIR}, endpoint = /batch\\n\")\n",
    "\n",
    "    for d in _daterange_inclusive(start_dt, end_dt):\n",
    "        total_days += 1\n",
    "        day_iso = d.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "        payload = _load_fallback_for_day(day_iso)\n",
    "        if not payload:\n",
    "            print(f\"[skip] no valid fallback rows for {day_iso}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        total_rows_in_file = len(payload)\n",
    "\n",
    "        # Hygiene (dedupe, drop same-currency, already normalized date/quotation/rate)\n",
    "        deduped_payload, dupes_removed = _dedupe_payload(payload)\n",
    "        total_dupes_removed += dupes_removed\n",
    "\n",
    "        filtered_payload, same_drop = _filter_same_currency(deduped_payload)\n",
    "        total_same_currency_dropped += same_drop\n",
    "\n",
    "        n_after = len(filtered_payload)\n",
    "        print(f\"[day] rows: {total_rows_in_file} → {n_after} \"\n",
    "              f\"(dedup removed {dupes_removed}, same-currency dropped {same_drop})\")\n",
    "\n",
    "        if n_after == 0:\n",
    "            _write_day_summary(\n",
    "                day_iso,\n",
    "                total_rows=total_rows_in_file,\n",
    "                rows_after_filters=n_after,\n",
    "                dupes_removed=dupes_removed,\n",
    "                same_currency_dropped=same_drop,\n",
    "                posted_batches=0,\n",
    "                rows_sent=0,\n",
    "                errors=0,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        chunks = _chunked(filtered_payload, bs)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"[day] batching: {total_chunks} batch(es) of up to {bs}\")\n",
    "\n",
    "        day_had_success = False\n",
    "        day_errors = 0\n",
    "        day_rows_sent = 0\n",
    "        day_batches_posted = 0\n",
    "\n",
    "        for idx, batch in enumerate(chunks, start=1):\n",
    "            print(f\"[post] {day_iso} | batch {idx}/{total_chunks}: {len(batch)} rows\")\n",
    "            ok, resp_json, resp_text = _post_payload_batch(batch)\n",
    "            if ok:\n",
    "                day_had_success = True\n",
    "                day_batches_posted += 1\n",
    "                posted_batches += 1\n",
    "                day_rows_sent += len(batch)\n",
    "                rows_sent += len(batch)\n",
    "                # Optionally inspect resp_json here\n",
    "            else:\n",
    "                day_errors += 1\n",
    "                errors += 1\n",
    "                print(f\"[error] POST failed for {day_iso} batch {idx}/{total_chunks}:\\n\"\n",
    "                      f\"{(resp_text or '(no body)')[:1000]}\")\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting at this batch\")\n",
    "                    _write_day_summary(\n",
    "                        day_iso,\n",
    "                        total_rows=total_rows_in_file,\n",
    "                        rows_after_filters=n_after,\n",
    "                        dupes_removed=dupes_removed,\n",
    "                        same_currency_dropped=same_drop,\n",
    "                        posted_batches=day_batches_posted,\n",
    "                        rows_sent=day_rows_sent,\n",
    "                        errors=day_errors,\n",
    "                    )\n",
    "                    # Final tallies before exit\n",
    "                    print(\"\\n[summary]\")\n",
    "                    print(f\"  days in range         : {total_days}\")\n",
    "                    print(f\"  posted days           : {posted_days}\")\n",
    "                    print(f\"  posted batches        : {posted_batches}\")\n",
    "                    print(f\"  rows sent             : {rows_sent}\")\n",
    "                    print(f\"  skipped days          : {skipped_days}\")\n",
    "                    print(f\"  errors                : {errors}\")\n",
    "                    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "                    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "                    return\n",
    "\n",
    "        if day_had_success:\n",
    "            posted_days += 1\n",
    "\n",
    "        _write_day_summary(\n",
    "            day_iso,\n",
    "            total_rows=total_rows_in_file,\n",
    "            rows_after_filters=n_after,\n",
    "            dupes_removed=dupes_removed,\n",
    "            same_currency_dropped=same_drop,\n",
    "            posted_batches=day_batches_posted,\n",
    "            rows_sent=day_rows_sent,\n",
    "            errors=day_errors,\n",
    "        )\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  days in range         : {total_days}\")\n",
    "    print(f\"  posted days           : {posted_days}\")\n",
    "    print(f\"  posted batches        : {posted_batches}\")\n",
    "    print(f\"  rows sent             : {rows_sent}\")\n",
    "    print(f\"  skipped days          : {skipped_days}\")\n",
    "    print(f\"  errors                : {errors}\")\n",
    "    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
