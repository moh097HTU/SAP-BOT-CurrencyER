{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a9d6d8",
   "metadata": {},
   "source": [
    "## call with interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2dc5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-07-01 → 2025-11-11 (inclusive)\n",
      "[config] batch_size = 400, endpoint = /batch\n",
      "[mode] STRICT ORDER: per-day, then per-day batches (no interleaving)\n",
      "\n",
      "\n",
      "=== DAY 2025-07-01 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-01 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-01. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-02 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-02 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-02. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-03 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-03 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-03. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-04 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-04 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-04. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-05 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-05 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-05. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-06 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-06 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-06. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-07 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-07 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-07. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-08 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-08 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-08. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-09 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-09 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-09. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-10 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-10 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-10. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-11 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-11 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-11. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-12 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-12 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-12. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-13 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-13 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-13. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-14 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-14 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-14. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-15 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-15 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-15. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-16 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-16 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-16. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-17 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-17 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-17. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-18 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-18 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-18. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-19 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-19 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-19. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-20 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-20 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-20. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-21 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-21 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-21. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-22 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-22 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-22. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-23 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-23 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-23. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-24 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-24 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-24. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-25 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-25 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-25. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-26 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-26 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-26. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-27 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-27 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-27. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-28 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-28 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-28. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-29 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-29 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-29. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-30 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-30 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-30. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-07-31 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-07-31 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-07-31. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-01 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-01 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-01. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-02 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-02 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-02. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-03 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-03 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-03. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-04 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-04 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-04. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-05 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-05 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-05. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-06 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-06 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-06. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-07 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-07 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-07. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-08 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-08 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-08. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-09 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-09 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-09. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-10 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-10 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-10. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-11 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-11 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-11. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-12 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-12 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-12. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-13 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-13 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-13. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-14 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-14 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-14. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-15 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-15 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-15. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-16 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-16 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-16. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-17 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-17 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-17. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-18 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-18 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-18. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-19 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-19 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-19. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-20 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-20 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-20. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-21 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-21 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-21. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-22 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-22 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-22. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-23 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-23 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-23. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-24 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-24 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-24. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-25 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-25 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-25. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-26 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-26 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-26. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-27 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-27 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-27. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-28 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-28 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-28. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-29 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-29 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-29. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-30 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-30 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-30. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-08-31 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-08-31 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-08-31. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-01 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-01 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-01. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-02 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-02 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-02. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-03 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-03 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-03. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-04 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-04 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-04. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-05 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-05 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-05. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-06 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-06 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-06. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-07 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-07 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-07. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-08 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-08 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-08. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-09 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-09 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-09. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-10 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-10 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-10. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-11 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-11 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-11. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-12 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-12 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-12. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-13 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-13 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-13. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-14 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-14 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-14. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-15 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-15 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-15. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-16 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-16 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-16. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-17 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-17 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-17. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-18 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-18 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-18. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-19 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-19 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-19. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-20 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-20 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-20. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-21 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-21 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-21. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-22 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-22 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-22. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-23 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-23 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-23. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-24 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-24 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-24. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-25 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-25 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-25. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-26 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-26 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-26. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-27 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-27 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-27. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-28 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-28 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-28. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-29 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-29 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-29. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-09-30 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-09-30 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-09-30. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-01 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-01 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-01. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-02 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-02 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-02. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-03 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-03 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-03. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-04 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-04 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-04. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-05 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-05 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-05. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-06 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-06 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-06. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-07 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-07 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-07. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-08 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-08 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-08. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-09 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-09 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-09. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-10 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-10 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-10. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-11 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-11 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-11. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-12 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-12 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-12. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-13 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-13 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-13. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-14 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-14 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-14. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-15 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-15 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-15. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-16 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-16 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-16. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-17 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-17 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-17. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-18 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-18 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-18. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-19 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-19 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-19. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-20 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-20 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-20. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-21 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-21 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-21. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-22 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-22 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-22. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-23 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-23 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-23. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-24 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-24 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-24. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-25 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-25 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-25. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-26 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-26 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-26. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-27 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-27 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-27. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-28 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-28 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-28. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-29 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-29 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-29. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-30 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-30 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-30. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-10-31 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-10-31 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-10-31. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-01 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-01 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-01. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-02 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-02 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-02. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-03 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-03 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-03. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-04 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-04 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-04. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-05 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-05 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-05. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-06 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-06 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-06. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-07 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-07 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-07. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-08 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-08 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-08. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-09 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-09 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-09. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-10 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-10 | batch 1/1: 4 rows → /batch\n",
      "[sleep] finished 2025-11-10. Waiting 120s before next day…\n",
      "\n",
      "=== DAY 2025-11-11 ===\n",
      "[day] rows: 4 → 4 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 400\n",
      "[post] 2025-11-11 | batch 1/1: 4 rows → /batch\n",
      "\n",
      "[summary]\n",
      "  days in range         : 134\n",
      "  days with payload     : 134\n",
      "  posted days           : 134\n",
      "  posted batches        : 134\n",
      "  rows sent             : 536\n",
      "  skipped days          : 0\n",
      "  errors                : 0\n",
      "  duplicates removed    : 0\n",
      "  same-currency dropped : 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG (non-streaming only)\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_BATCH = f\"{API_BASE}/currency/exchange-rates/batch\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD for selecting folders (folders are YYYY-MM-DD)\n",
    "START_DATE = \"01-07-2025\"\n",
    "END_DATE   = \"11-11-2025\"\n",
    "\n",
    "# Batch size per request (set to 20 by default; change as needed)\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 300  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # if True, stop immediately when a batch fails\n",
    "\n",
    "# Input hygiene\n",
    "DROP_SAME_CURRENCY = True  # drop items where FromCurrency == ToCurrency\n",
    "\n",
    "# Output\n",
    "WRITE_DAY_SUMMARY = True  # write per-day JSON summary under each day folder\n",
    "\n",
    "# Throttling between days\n",
    "WAIT_BETWEEN_DAYS_SECONDS = 120  # ⏱ wait 2 minutes after finishing a day before next day\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _load_payload_for_day(day_dir: str) -> Optional[List[Any]]:\n",
    "    \"\"\"Read exchange_rates_payload.json and return AS-IS (no normalization).\"\"\"\n",
    "    path = os.path.join(day_dir, \"exchange_rates_payload.json\")\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[skip] payload not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            print(f\"[warn] payload is not a list, skipping: {path}\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] failed reading payload {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _chunked(lst: List[Any], size: int) -> List[List[Any]]:\n",
    "    size = max(1, int(size))\n",
    "    return [lst[i:i+size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def _dedupe_payload(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Deduplicate by logical key; fallback to full JSON. Preserve order.\"\"\"\n",
    "    seen = set()\n",
    "    out: List[Any] = []\n",
    "    removed = 0\n",
    "    for item in payload:\n",
    "        key: Optional[Tuple[Any, ...]] = None\n",
    "        if isinstance(item, dict):\n",
    "            fields = (\n",
    "                item.get(\"ExchangeRateType\"),\n",
    "                item.get(\"FromCurrency\"),\n",
    "                item.get(\"ToCurrency\"),\n",
    "                item.get(\"ValidFrom\"),\n",
    "                item.get(\"Quotation\"),\n",
    "            )\n",
    "            if all(v is not None for v in fields):\n",
    "                key = (\"LOGIC_KEY\",) + fields\n",
    "        if key is None:\n",
    "            try:\n",
    "                canonical = json.dumps(item, sort_keys=True, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                canonical = repr(item)\n",
    "            key = (\"RAW_ITEM\", canonical)\n",
    "        if key in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(item)\n",
    "    return out, removed\n",
    "\n",
    "def _filter_same_currency(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Drop rows where FromCurrency == ToCurrency (exact string compare).\"\"\"\n",
    "    out: List[Any] = []\n",
    "    dropped = 0\n",
    "    for it in payload:\n",
    "        if isinstance(it, dict):\n",
    "            f = it.get(\"FromCurrency\")\n",
    "            t = it.get(\"ToCurrency\")\n",
    "            if f is not None and t is not None and str(f) == str(t):\n",
    "                dropped += 1\n",
    "                continue\n",
    "        out.append(it)\n",
    "    return out, dropped\n",
    "\n",
    "def _post_with_retries(url: str, *, json_body: Any, timeout: Optional[int]) -> requests.Response:\n",
    "    \"\"\"POST with exponential backoff on 429/5xx/connect/read issues.\"\"\"\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, json=json_body, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_payload_batch(payload: List[Any]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"Always post to the non-streaming /batch endpoint.\"\"\"\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_BATCH, json_body=payload, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body = e.response.text\n",
    "            except Exception:\n",
    "                body = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body) if body else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, *, total_rows: int, rows_after_filters: int,\n",
    "                       dupes_removed: int, same_currency_dropped: int,\n",
    "                       posted_batches: int, rows_sent: int, errors: int) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"total_rows_in_file\": total_rows,\n",
    "            \"rows_after_filters\": rows_after_filters,\n",
    "            \"duplicates_removed\": dupes_removed,\n",
    "            \"same_currency_dropped\": same_currency_dropped,\n",
    "            \"posted_batches\": posted_batches,\n",
    "            \"rows_sent\": rows_sent,\n",
    "            \"errors\": errors,\n",
    "            \"endpoint\": \"batch\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(day_dir, \"post_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN (strict day->batches sequence)\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    bs = max(1, int(BATCH_SIZE))\n",
    "\n",
    "    total_days = 0\n",
    "    days_with_payload = 0\n",
    "    posted_days = 0\n",
    "    posted_batches = 0\n",
    "    rows_sent = 0\n",
    "    skipped_days = 0\n",
    "    errors = 0\n",
    "    total_dupes_removed = 0\n",
    "    total_same_currency_dropped = 0\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] batch_size = {bs}, endpoint = /batch\")\n",
    "    print(\"[mode] STRICT ORDER: per-day, then per-day batches (no interleaving)\\n\")\n",
    "\n",
    "    # STRICT: iterate days in order\n",
    "    for d in _daterange_inclusive(start_dt, end_dt):\n",
    "        total_days += 1\n",
    "        day_name = d.strftime(\"%Y-%m-%d\")\n",
    "        day_dir = os.path.join(BASE_DIR, day_name)\n",
    "        print(f\"\\n=== DAY {day_name} ===\")\n",
    "\n",
    "        if not os.path.isdir(day_dir):\n",
    "            print(f\"[skip] day folder missing: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        payload = _load_payload_for_day(day_dir)\n",
    "        if not payload:\n",
    "            print(f\"[skip] no valid payload in: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        days_with_payload += 1\n",
    "        total_rows_in_file = len(payload)\n",
    "\n",
    "        # Hygiene for the day (still AS-IS structure)\n",
    "        deduped_payload, dupes_removed = _dedupe_payload(payload)\n",
    "        total_dupes_removed += dupes_removed\n",
    "\n",
    "        if DROP_SAME_CURRENCY:\n",
    "            filtered_payload, same_drop = _filter_same_currency(deduped_payload)\n",
    "        else:\n",
    "            filtered_payload, same_drop = deduped_payload, 0\n",
    "        total_same_currency_dropped += same_drop\n",
    "\n",
    "        n_after = len(filtered_payload)\n",
    "        print(f\"[day] rows: {total_rows_in_file} → {n_after} \"\n",
    "              f\"(dedup removed {dupes_removed}, same-currency dropped {same_drop})\")\n",
    "\n",
    "        if n_after == 0:\n",
    "            print(f\"[day] {day_name}: nothing to post after filters; skipping\")\n",
    "            _write_day_summary(day_dir,\n",
    "                               total_rows=total_rows_in_file,\n",
    "                               rows_after_filters=n_after,\n",
    "                               dupes_removed=dupes_removed,\n",
    "                               same_currency_dropped=same_drop,\n",
    "                               posted_batches=0,\n",
    "                               rows_sent=0,\n",
    "                               errors=0)\n",
    "            continue\n",
    "\n",
    "        # STRICT: process this day's batches sequentially\n",
    "        chunks = _chunked(filtered_payload, bs)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"[day] batching: {total_chunks} batch(es) of up to {bs}\")\n",
    "\n",
    "        day_had_success = False\n",
    "        day_errors = 0\n",
    "        day_rows_sent = 0\n",
    "        day_batches_posted = 0\n",
    "\n",
    "        for idx, batch in enumerate(chunks, start=1):\n",
    "            print(f\"[post] {day_name} | batch {idx}/{total_chunks}: {len(batch)} rows → /batch\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_payload_batch(batch)\n",
    "\n",
    "            if ok:\n",
    "                day_had_success = True\n",
    "                day_batches_posted += 1\n",
    "                posted_batches += 1\n",
    "                day_rows_sent += len(batch)\n",
    "                rows_sent += len(batch)\n",
    "\n",
    "                if resp_json is not None:\n",
    "                    # Uncomment for verbose:\n",
    "                    # print(json.dumps(resp_json, indent=2, ensure_ascii=False)[:2000])\n",
    "                    pass\n",
    "                elif resp_text is not None:\n",
    "                    print(resp_text[:1000])\n",
    "                else:\n",
    "                    print(\"[info] posted OK (no response body)\")\n",
    "            else:\n",
    "                day_errors += 1\n",
    "                errors += 1\n",
    "                print(f\"[error] POST failed for {day_name} batch {idx}/{total_chunks}:\\n\"\n",
    "                      f\"{(resp_text or '(no body)')[:1000]}\")\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting at this batch\")\n",
    "                    # write partial day summary then stop everything\n",
    "                    _write_day_summary(day_dir,\n",
    "                                       total_rows=total_rows_in_file,\n",
    "                                       rows_after_filters=n_after,\n",
    "                                       dupes_removed=dupes_removed,\n",
    "                                       same_currency_dropped=same_drop,\n",
    "                                       posted_batches=day_batches_posted,\n",
    "                                       rows_sent=day_rows_sent,\n",
    "                                       errors=day_errors)\n",
    "                    print(\"\\n[summary]\")\n",
    "                    print(f\"  days in range         : {total_days}\")\n",
    "                    print(f\"  days with payload     : {days_with_payload}\")\n",
    "                    print(f\"  posted days           : {posted_days}\")\n",
    "                    print(f\"  posted batches        : {posted_batches}\")\n",
    "                    print(f\"  rows sent             : {rows_sent}\")\n",
    "                    print(f\"  skipped days          : {skipped_days}\")\n",
    "                    print(f\"  errors                : {errors}\")\n",
    "                    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "                    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "                    return\n",
    "\n",
    "        if day_had_success:\n",
    "            posted_days += 1\n",
    "\n",
    "        _write_day_summary(day_dir,\n",
    "                           total_rows=total_rows_in_file,\n",
    "                           rows_after_filters=n_after,\n",
    "                           dupes_removed=dupes_removed,\n",
    "                           same_currency_dropped=same_drop,\n",
    "                           posted_batches=day_batches_posted,\n",
    "                           rows_sent=day_rows_sent,\n",
    "                           errors=day_errors)\n",
    "\n",
    "        # =========================\n",
    "        # WAIT before next day\n",
    "        # =========================\n",
    "        if day_batches_posted > 0 and d < end_dt:\n",
    "            print(f\"[sleep] finished {day_name}. Waiting {WAIT_BETWEEN_DAYS_SECONDS}s before next day…\")\n",
    "            time.sleep(WAIT_BETWEEN_DAYS_SECONDS)\n",
    "\n",
    "    # Final overall summary\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  days in range         : {total_days}\")\n",
    "    print(f\"  days with payload     : {days_with_payload}\")\n",
    "    print(f\"  posted days           : {posted_days}\")\n",
    "    print(f\"  posted batches        : {posted_batches}\")\n",
    "    print(f\"  rows sent             : {rows_sent}\")\n",
    "    print(f\"  skipped days          : {skipped_days}\")\n",
    "    print(f\"  errors                : {errors}\")\n",
    "    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31421a",
   "metadata": {},
   "source": [
    "## Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4331ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-11-30 → 2025-11-30 (inclusive)\n",
      "[config] batch_size = 408, endpoint = /batch\n",
      "[mode] STRICT ORDER: per-day, then per-day batches (no interleaving)\n",
      "\n",
      "\n",
      "=== DAY 2025-11-30 ===\n",
      "[day] rows: 408 → 408 (dedup removed 0, same-currency dropped 0)\n",
      "[day] batching: 1 batch(es) of up to 408\n",
      "[post] 2025-11-30 | batch 1/1: 408 rows → /batch\n",
      "\n",
      "[summary]\n",
      "  days in range         : 1\n",
      "  days with payload     : 1\n",
      "  posted days           : 1\n",
      "  posted batches        : 1\n",
      "  rows sent             : 408\n",
      "  skipped days          : 0\n",
      "  errors                : 0\n",
      "  duplicates removed    : 0\n",
      "  same-currency dropped : 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG (non-streaming only)\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_BATCH = f\"{API_BASE}/currency/exchange-rates/batch\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD for selecting folders (folders are YYYY-MM-DD)\n",
    "START_DATE = \"30-11-2025\"\n",
    "END_DATE   = \"30-11-2025\"\n",
    "\n",
    "# Batch size per request (set to 20 by default; change as needed)\n",
    "BATCH_SIZE = 408\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 30000  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # if True, stop immediately when a batch fails\n",
    "\n",
    "# Input hygiene\n",
    "DROP_SAME_CURRENCY = True  # drop items where FromCurrency == ToCurrency\n",
    "\n",
    "# Output\n",
    "WRITE_DAY_SUMMARY = True  # write per-day JSON summary under each day folder\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _load_payload_for_day(day_dir: str) -> Optional[List[Any]]:\n",
    "    \"\"\"Read exchange_rates_payload.json and return AS-IS (no normalization).\"\"\"\n",
    "    path = os.path.join(day_dir, \"exchange_rates_payload.json\")\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[skip] payload not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            print(f\"[warn] payload is not a list, skipping: {path}\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] failed reading payload {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _chunked(lst: List[Any], size: int) -> List[List[Any]]:\n",
    "    size = max(1, int(size))\n",
    "    return [lst[i:i+size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def _dedupe_payload(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Deduplicate by logical key; fallback to full JSON. Preserve order.\"\"\"\n",
    "    seen = set()\n",
    "    out: List[Any] = []\n",
    "    removed = 0\n",
    "    for item in payload:\n",
    "        key: Optional[Tuple[Any, ...]] = None\n",
    "        if isinstance(item, dict):\n",
    "            fields = (\n",
    "                item.get(\"ExchangeRateType\"),\n",
    "                item.get(\"FromCurrency\"),\n",
    "                item.get(\"ToCurrency\"),\n",
    "                item.get(\"ValidFrom\"),\n",
    "                item.get(\"Quotation\"),\n",
    "            )\n",
    "            if all(v is not None for v in fields):\n",
    "                key = (\"LOGIC_KEY\",) + fields\n",
    "        if key is None:\n",
    "            try:\n",
    "                canonical = json.dumps(item, sort_keys=True, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                canonical = repr(item)\n",
    "            key = (\"RAW_ITEM\", canonical)\n",
    "        if key in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(item)\n",
    "    return out, removed\n",
    "\n",
    "def _filter_same_currency(payload: List[Any]) -> Tuple[List[Any], int]:\n",
    "    \"\"\"Drop rows where FromCurrency == ToCurrency (exact string compare).\"\"\"\n",
    "    out: List[Any] = []\n",
    "    dropped = 0\n",
    "    for it in payload:\n",
    "        if isinstance(it, dict):\n",
    "            f = it.get(\"FromCurrency\")\n",
    "            t = it.get(\"ToCurrency\")\n",
    "            if f is not None and t is not None and str(f) == str(t):\n",
    "                dropped += 1\n",
    "                continue\n",
    "        out.append(it)\n",
    "    return out, dropped\n",
    "\n",
    "def _post_with_retries(url: str, *, json_body: Any, timeout: Optional[int]) -> requests.Response:\n",
    "    \"\"\"POST with exponential backoff on 429/5xx/connect/read issues.\"\"\"\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, json=json_body, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_payload_batch(payload: List[Any]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"Always post to the non-streaming /batch endpoint.\"\"\"\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_BATCH, json_body=payload, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body = e.response.text\n",
    "            except Exception:\n",
    "                body = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body) if body else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, *, total_rows: int, rows_after_filters: int,\n",
    "                       dupes_removed: int, same_currency_dropped: int,\n",
    "                       posted_batches: int, rows_sent: int, errors: int) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"total_rows_in_file\": total_rows,\n",
    "            \"rows_after_filters\": rows_after_filters,\n",
    "            \"duplicates_removed\": dupes_removed,\n",
    "            \"same_currency_dropped\": same_currency_dropped,\n",
    "            \"posted_batches\": posted_batches,\n",
    "            \"rows_sent\": rows_sent,\n",
    "            \"errors\": errors,\n",
    "            \"endpoint\": \"batch\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(day_dir, \"post_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN (strict day->batches sequence)\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    bs = max(1, int(BATCH_SIZE))\n",
    "\n",
    "    total_days = 0\n",
    "    days_with_payload = 0\n",
    "    posted_days = 0\n",
    "    posted_batches = 0\n",
    "    rows_sent = 0\n",
    "    skipped_days = 0\n",
    "    errors = 0\n",
    "    total_dupes_removed = 0\n",
    "    total_same_currency_dropped = 0\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] batch_size = {bs}, endpoint = /batch\")\n",
    "    print(\"[mode] STRICT ORDER: per-day, then per-day batches (no interleaving)\\n\")\n",
    "\n",
    "    # STRICT: iterate days in order\n",
    "    for d in _daterange_inclusive(start_dt, end_dt):\n",
    "        total_days += 1\n",
    "        day_name = d.strftime(\"%Y-%m-%d\")\n",
    "        day_dir = os.path.join(BASE_DIR, day_name)\n",
    "        print(f\"\\n=== DAY {day_name} ===\")\n",
    "\n",
    "        if not os.path.isdir(day_dir):\n",
    "            print(f\"[skip] day folder missing: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        payload = _load_payload_for_day(day_dir)\n",
    "        if not payload:\n",
    "            print(f\"[skip] no valid payload in: {day_dir}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        days_with_payload += 1\n",
    "        total_rows_in_file = len(payload)\n",
    "\n",
    "        # Hygiene for the day (still AS-IS structure)\n",
    "        deduped_payload, dupes_removed = _dedupe_payload(payload)\n",
    "        total_dupes_removed += dupes_removed\n",
    "\n",
    "        if DROP_SAME_CURRENCY:\n",
    "            filtered_payload, same_drop = _filter_same_currency(deduped_payload)\n",
    "        else:\n",
    "            filtered_payload, same_drop = deduped_payload, 0\n",
    "        total_same_currency_dropped += same_drop\n",
    "\n",
    "        n_after = len(filtered_payload)\n",
    "        print(f\"[day] rows: {total_rows_in_file} → {n_after} \"\n",
    "              f\"(dedup removed {dupes_removed}, same-currency dropped {same_drop})\")\n",
    "\n",
    "        if n_after == 0:\n",
    "            print(f\"[day] {day_name}: nothing to post after filters; skipping\")\n",
    "            _write_day_summary(day_dir,\n",
    "                               total_rows=total_rows_in_file,\n",
    "                               rows_after_filters=n_after,\n",
    "                               dupes_removed=dupes_removed,\n",
    "                               same_currency_dropped=same_drop,\n",
    "                               posted_batches=0,\n",
    "                               rows_sent=0,\n",
    "                               errors=0)\n",
    "            continue\n",
    "\n",
    "        # STRICT: process this day's batches sequentially\n",
    "        chunks = _chunked(filtered_payload, bs)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"[day] batching: {total_chunks} batch(es) of up to {bs}\")\n",
    "\n",
    "        day_had_success = False\n",
    "        day_errors = 0\n",
    "        day_rows_sent = 0\n",
    "        day_batches_posted = 0\n",
    "\n",
    "        for idx, batch in enumerate(chunks, start=1):\n",
    "            print(f\"[post] {day_name} | batch {idx}/{total_chunks}: {len(batch)} rows → /batch\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_payload_batch(batch)\n",
    "\n",
    "            if ok:\n",
    "                day_had_success = True\n",
    "                day_batches_posted += 1\n",
    "                posted_batches += 1\n",
    "                day_rows_sent += len(batch)\n",
    "                rows_sent += len(batch)\n",
    "\n",
    "                if resp_json is not None:\n",
    "                    # Uncomment for verbose:\n",
    "                    # print(json.dumps(resp_json, indent=2, ensure_ascii=False)[:2000])\n",
    "                    pass\n",
    "                elif resp_text is not None:\n",
    "                    print(resp_text[:1000])\n",
    "                else:\n",
    "                    print(\"[info] posted OK (no response body)\")\n",
    "            else:\n",
    "                day_errors += 1\n",
    "                errors += 1\n",
    "                print(f\"[error] POST failed for {day_name} batch {idx}/{total_chunks}:\\n\"\n",
    "                      f\"{(resp_text or '(no body)')[:1000]}\")\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting at this batch\")\n",
    "                    # write partial day summary then stop everything\n",
    "                    _write_day_summary(day_dir,\n",
    "                                       total_rows=total_rows_in_file,\n",
    "                                       rows_after_filters=n_after,\n",
    "                                       dupes_removed=dupes_removed,\n",
    "                                       same_currency_dropped=same_drop,\n",
    "                                       posted_batches=day_batches_posted,\n",
    "                                       rows_sent=day_rows_sent,\n",
    "                                       errors=day_errors)\n",
    "                    print(\"\\n[summary]\")\n",
    "                    print(f\"  days in range         : {total_days}\")\n",
    "                    print(f\"  days with payload     : {days_with_payload}\")\n",
    "                    print(f\"  posted days           : {posted_days}\")\n",
    "                    print(f\"  posted batches        : {posted_batches}\")\n",
    "                    print(f\"  rows sent             : {rows_sent}\")\n",
    "                    print(f\"  skipped days          : {skipped_days}\")\n",
    "                    print(f\"  errors                : {errors}\")\n",
    "                    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "                    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "                    return\n",
    "\n",
    "        if day_had_success:\n",
    "            posted_days += 1\n",
    "\n",
    "        _write_day_summary(day_dir,\n",
    "                           total_rows=total_rows_in_file,\n",
    "                           rows_after_filters=n_after,\n",
    "                           dupes_removed=dupes_removed,\n",
    "                           same_currency_dropped=same_drop,\n",
    "                           posted_batches=day_batches_posted,\n",
    "                           rows_sent=day_rows_sent,\n",
    "                           errors=day_errors)\n",
    "\n",
    "    # Final overall summary\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  days in range         : {total_days}\")\n",
    "    print(f\"  days with payload     : {days_with_payload}\")\n",
    "    print(f\"  posted days           : {posted_days}\")\n",
    "    print(f\"  posted batches        : {posted_batches}\")\n",
    "    print(f\"  rows sent             : {rows_sent}\")\n",
    "    print(f\"  skipped days          : {skipped_days}\")\n",
    "    print(f\"  errors                : {errors}\")\n",
    "    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b2007",
   "metadata": {},
   "source": [
    "# Draft Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe6f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-10-31 → 2025-10-31 (inclusive)\n",
      "[config] endpoint=/currency/exchange-rates/drafts/delete, per_day=True, max_per_day=None\n",
      "\n",
      "\n",
      "=== DAY 2025-10-31 ===\n",
      "[retry] attempt 1/4 failed: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /currency/exchange-rates/drafts/delete?date_from=2025-10-31&date_to=2025-10-31 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D14D6C01D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Backing off 1s\n",
      "[retry] attempt 2/4 failed: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /currency/exchange-rates/drafts/delete?date_from=2025-10-31&date_to=2025-10-31 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D14D6C0BC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Backing off 2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     conn = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m socket.error(\u001b[33m\"\u001b[39m\u001b[33mgetaddrinfo returns an empty list\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:716\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:416\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m         \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:244\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers)\u001b[39m\n\u001b[32m    243\u001b[39m     headers[\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m] = _get_default_user_agent()\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1319\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1318\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1365\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1364\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1314\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1074\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1077\u001b[39m \n\u001b[32m   1078\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1018\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     conn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_conn(conn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:186\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    187\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % e\n\u001b[32m    188\u001b[39m     )\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x000001D14D6C0BC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:802\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    800\u001b[39m     e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, e)\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:594\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_retry.is_exhausted():\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[32m    596\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /currency/exchange-rates/drafts/delete?date_from=2025-10-31&date_to=2025-10-31 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D14D6C0BC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36m_post_with_retries\u001b[39m\u001b[34m(url, params, timeout)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m429\u001b[39m, \u001b[32m502\u001b[39m, \u001b[32m503\u001b[39m, \u001b[32m504\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:637\u001b[39m, in \u001b[36mSession.post\u001b[39m\u001b[34m(self, url, data, json, **kwargs)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /currency/exchange-rates/drafts/delete?date_from=2025-10-31&date_to=2025-10-31 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D14D6C0BC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    266\u001b[39m         _write_range_summary(BASE_DIR,\n\u001b[32m    267\u001b[39m                              start_date=df,\n\u001b[32m    268\u001b[39m                              end_date=dt,\n\u001b[32m    269\u001b[39m                              total_deleted=total_deleted,\n\u001b[32m    270\u001b[39m                              days_processed=days_processed,\n\u001b[32m    271\u001b[39m                              errors=errors)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    180\u001b[39m day_dir = os.path.join(BASE_DIR, day_iso)\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== DAY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday_iso\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m ok, resp_json, resp_text = \u001b[43m_post_delete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_iso\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_iso\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_PER_DAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ok \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp_json, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m resp_json.get(\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    185\u001b[39m     days_processed += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36m_post_delete\u001b[39m\u001b[34m(day_from, day_to, max_per_day)\u001b[39m\n\u001b[32m    100\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mmax_per_day\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mint\u001b[39m(max_per_day)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     r = \u001b[43m_post_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL_DELETE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREQUEST_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, r.json(), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36m_post_with_retries\u001b[39m\u001b[34m(url, params, timeout)\u001b[39m\n\u001b[32m     85\u001b[39m         sleep_s = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(RETRY_BACKOFF_BASE) ** (attempt - \u001b[32m1\u001b[39m))\n\u001b[32m     86\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[retry] attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRETRY_MAX\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Backing off \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_s\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_exc, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# post_delete_drafts.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_DELETE = f\"{API_BASE}/currency/exchange-rates/drafts/delete\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD\n",
    "START_DATE = \"31-10-2025\"\n",
    "END_DATE   = \"31-10-2025\"\n",
    "\n",
    "# If True → call the API one day at a time (date_from=day=date_to).\n",
    "# If False → call the API once with the full inclusive range.\n",
    "PER_DAY = True\n",
    "\n",
    "# Optional cap on deletions per day. Set to None to remove the limit.\n",
    "MAX_PER_DAY: Optional[int] = None  # e.g., 200\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 300  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # if True, halt immediately on a failed POST\n",
    "\n",
    "# Output\n",
    "WRITE_DAY_SUMMARY = True   # write per-day JSON summary under each day folder\n",
    "WRITE_RANGE_SUMMARY = True # write a summary json under BASE_DIR when PER_DAY=False\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _post_with_retries(url: str, *, params: Dict[str, Any], timeout: Optional[int]) -> requests.Response:\n",
    "    \"\"\"POST with exponential backoff on 429/5xx/connect/read issues.\"\"\"\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, params=params, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_delete(day_from: str, day_to: str, max_per_day: Optional[int]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Calls /currency/exchange-rates/drafts/delete using query params:\n",
    "      ?date_from=YYYY-MM-DD&date_to=YYYY-MM-DD&max_per_day=INT\n",
    "    If date_from == date_to, server processes one day only (as required).\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\"date_from\": day_from, \"date_to\": day_to}\n",
    "    if max_per_day is not None:\n",
    "        params[\"max_per_day\"] = int(max_per_day)\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_DELETE, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body_txt = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body_txt = e.response.text\n",
    "            except Exception:\n",
    "                body_txt = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body_txt) if body_txt else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, *, deleted_count: int, deleted_sample: Optional[list], ok: bool, why: str = \"\") -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"deleted_count\": int(deleted_count),\n",
    "            \"deleted_sample\": deleted_sample or [],\n",
    "            \"ok\": bool(ok),\n",
    "            \"why\": why,\n",
    "            \"endpoint\": \"drafts/delete\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        _ensure_dir(day_dir)\n",
    "        p = os.path.join(day_dir, \"delete_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "def _write_range_summary(base_dir: str, *, start_date: str, end_date: str, total_deleted: int, days_processed: int, errors: int) -> None:\n",
    "    if not WRITE_RANGE_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"range\": {\"from\": start_date, \"to\": end_date},\n",
    "            \"total_deleted\": int(total_deleted),\n",
    "            \"days_processed\": int(days_processed),\n",
    "            \"errors\": int(errors),\n",
    "            \"endpoint\": \"drafts/delete\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(base_dir, \"delete_summary_range.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write range summary failed: {e}\")\n",
    "\n",
    "def _iso(d: datetime) -> str:\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] endpoint=/currency/exchange-rates/drafts/delete, per_day={PER_DAY}, max_per_day={MAX_PER_DAY}\\n\")\n",
    "\n",
    "    if PER_DAY:\n",
    "        # ---------- one API call per day ----------\n",
    "        total_days = 0\n",
    "        days_processed = 0\n",
    "        total_deleted = 0\n",
    "        skipped_days = 0\n",
    "        errors = 0\n",
    "\n",
    "        for d in _daterange_inclusive(start_dt, end_dt):\n",
    "            total_days += 1\n",
    "            day_iso = _iso(d)\n",
    "            day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "            print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_delete(day_iso, day_iso, MAX_PER_DAY)\n",
    "            if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "                days_processed += 1\n",
    "                deleted_count = int(resp_json.get(\"total_deleted\", 0))\n",
    "                total_deleted += deleted_count\n",
    "                deleted_sample = []\n",
    "                # Try to collect a small sample from 'per_day' entry in server payload\n",
    "                try:\n",
    "                    per_day = resp_json.get(\"per_day\") or []\n",
    "                    if isinstance(per_day, list):\n",
    "                        for entry in per_day:\n",
    "                            if isinstance(entry, dict) and entry.get(\"date\") == day_iso:\n",
    "                                deleted_sample = entry.get(\"sample\", []) or []\n",
    "                                break\n",
    "                except Exception:\n",
    "                    deleted_sample = []\n",
    "\n",
    "                print(f\"[day] deleted: {deleted_count}\")\n",
    "                _write_day_summary(day_dir,\n",
    "                                deleted_count=deleted_count,\n",
    "                                deleted_sample=deleted_sample[:10],\n",
    "                                ok=True)\n",
    "            else:\n",
    "                errors += 1\n",
    "                why = \"\"\n",
    "                if resp_json and isinstance(resp_json, dict):\n",
    "                    why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "                elif resp_text:\n",
    "                    why = str(resp_text)[:400]\n",
    "                print(f\"[error] delete failed for {day_iso}: {why or '(no details)'}\")\n",
    "                _write_day_summary(day_dir, deleted_count=0, deleted_sample=[], ok=False, why=why)\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "                    _write_range_summary(BASE_DIR,\n",
    "                                         start_date=_iso(start_dt),\n",
    "                                         end_date=_iso(end_dt),\n",
    "                                         total_deleted=total_deleted,\n",
    "                                         days_processed=days_processed,\n",
    "                                         errors=errors)\n",
    "                    return\n",
    "                skipped_days += 1\n",
    "\n",
    "        # Final summary\n",
    "        print(\"\\n[summary]\")\n",
    "        print(f\"  days in range   : {total_days}\")\n",
    "        print(f\"  processed days  : {days_processed}\")\n",
    "        print(f\"  total deleted   : {total_deleted}\")\n",
    "        print(f\"  skipped days    : {skipped_days}\")\n",
    "        print(f\"  errors          : {errors}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=_iso(start_dt),\n",
    "                             end_date=_iso(end_dt),\n",
    "                             total_deleted=total_deleted,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "    else:\n",
    "        # ---------- one API call for the entire range ----------\n",
    "        df = _iso(start_dt)\n",
    "        dt = _iso(end_dt)\n",
    "        print(f\"[range] single call: {df}..{dt}\")\n",
    "        ok, resp_json, resp_text = _post_delete(df, dt, MAX_PER_DAY)\n",
    "\n",
    "        total_deleted = 0\n",
    "        days_processed = 0\n",
    "        errors = 0\n",
    "\n",
    "        if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "            total_deleted = int(resp_json.get(\"total_deleted\", 0))\n",
    "            days_processed = int(resp_json.get(\"days_processed\", 0))\n",
    "            print(f\"[range] deleted={total_deleted}, days_processed={days_processed}\")\n",
    "        else:\n",
    "            errors = 1\n",
    "            why = \"\"\n",
    "            if resp_json and isinstance(resp_json, dict):\n",
    "                why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "            elif resp_text:\n",
    "                why = str(resp_text)[:400]\n",
    "            print(f\"[error] range delete failed: {why or '(no details)'}\")\n",
    "            if STOP_ON_ERROR:\n",
    "                print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=df,\n",
    "                             end_date=dt,\n",
    "                             total_deleted=total_deleted,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e20ba",
   "metadata": {},
   "source": [
    "# collect_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6c3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-11-23 → 2025-11-23 (inclusive)\n",
      "[config] endpoint=/currency/exchange-rates/fallback/collect-missing, per_day=True\n",
      "\n",
      "\n",
      "=== DAY 2025-11-23 ===\n",
      "[day] excel_rows=253 json_rows=408 missing=155\n",
      "\n",
      "[summary]\n",
      "  processed days  : 1\n",
      "  total missing   : 155\n",
      "  errors          : 0\n"
     ]
    }
   ],
   "source": [
    "# collect_missing.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL = f\"{API_BASE}/currency/exchange-rates/fallback/collect-missing\"\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD\n",
    "START_DATE = \"23-11-2025\"\n",
    "END_DATE   = \"23-11-2025\"\n",
    "\n",
    "PER_DAY = True\n",
    "REQUEST_TIMEOUT = 300\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2\n",
    "STOP_ON_ERROR = False\n",
    "WRITE_DAY_SUMMARY = True\n",
    "WRITE_RANGE_SUMMARY = True\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _post_with_retries(url: str, *, params: Dict[str, Any], timeout: Optional[int]) -> requests.Response:\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, params=params, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_collect(day_from: str, day_to: str) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    params: Dict[str, Any] = {\"date_from\": day_from, \"date_to\": day_to}\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body_txt = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body_txt = e.response.text\n",
    "            except Exception:\n",
    "                body_txt = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body_txt) if body_txt else ''}\"\n",
    "\n",
    "def _write_day_summary(day_dir: str, payload: Dict[str, Any]) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        _ensure_dir(day_dir)\n",
    "        p = os.path.join(day_dir, \"fallback_collect_summary.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write day summary failed in {day_dir}: {e}\")\n",
    "\n",
    "def _write_range_summary(base_dir: str, *, start_date: str, end_date: str, total_missing: int, days_processed: int, errors: int) -> None:\n",
    "    if not WRITE_RANGE_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"range\": {\"from\": start_date, \"to\": end_date},\n",
    "            \"total_missing\": int(total_missing),\n",
    "            \"days_processed\": int(days_processed),\n",
    "            \"errors\": int(errors),\n",
    "            \"endpoint\": \"fallback/collect-missing\",\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        p = os.path.join(base_dir, \"fallback_collect_summary_range.json\")\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write range summary failed: {e}\")\n",
    "\n",
    "def _iso(d: datetime) -> str:\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] endpoint=/currency/exchange-rates/fallback/collect-missing, per_day={PER_DAY}\\n\")\n",
    "\n",
    "    total_missing = 0\n",
    "    days_processed = 0\n",
    "    errors = 0\n",
    "\n",
    "    if PER_DAY:\n",
    "        for d in _daterange_inclusive(start_dt, end_dt):\n",
    "            day_iso = _iso(d)\n",
    "            day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "            print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "            ok, resp_json, resp_text = _post_collect(day_iso, day_iso)\n",
    "            if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "                # grab the day row (if present)\n",
    "                per_day = resp_json.get(\"per_day\") or []\n",
    "                found = None\n",
    "                for entry in per_day:\n",
    "                    if isinstance(entry, dict) and entry.get(\"date\") == day_iso:\n",
    "                        found = entry\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    days_processed += 1\n",
    "                    total_missing += int(found.get(\"missing\", 0))\n",
    "                    print(f\"[day] excel_rows={found.get('excel_rows')} json_rows={found.get('json_rows')} missing={found.get('missing')}\")\n",
    "                    _write_day_summary(day_dir, {\n",
    "                        \"missing_count\": found.get(\"missing\", 0),\n",
    "                        \"excel_rows\": found.get(\"excel_rows\", 0),\n",
    "                        \"json_rows\": found.get(\"json_rows\", 0),\n",
    "                        \"tracker_path\": found.get(\"tracker_path\", \"\"),\n",
    "                        \"export_clicked\": found.get(\"export_clicked\", False),\n",
    "                        \"xlsx_path\": found.get(\"xlsx_path\", \"\"),\n",
    "                        \"xlsx_size\": found.get(\"xlsx_size\", 0),\n",
    "                        \"headers_seen\": found.get(\"headers_seen\", []),\n",
    "                        \"json_file_exists\": found.get(\"json_file_exists\", False),\n",
    "                        \"ok\": True,\n",
    "                        \"why\": \"\",\n",
    "                        \"endpoint\": \"fallback/collect-missing\",\n",
    "                        \"ts\": datetime.now().isoformat(),\n",
    "                    })\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    print(f\"[warn] API ok but 'per_day' did not contain {day_iso}\")\n",
    "                    _write_day_summary(day_dir, {\n",
    "                        \"missing_count\": 0, \"excel_rows\": 0, \"json_rows\": 0,\n",
    "                        \"tracker_path\": \"\", \"export_clicked\": False, \"xlsx_path\": \"\",\n",
    "                        \"xlsx_size\": 0, \"headers_seen\": [], \"json_file_exists\": False,\n",
    "                        \"ok\": False, \"why\": \"no_day_entry_in_response\",\n",
    "                        \"endpoint\": \"fallback/collect-missing\", \"ts\": datetime.now().isoformat(),\n",
    "                    })\n",
    "\n",
    "            else:\n",
    "                errors += 1\n",
    "                why = \"\"\n",
    "                if resp_json and isinstance(resp_json, dict):\n",
    "                    why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "                elif resp_text:\n",
    "                    why = str(resp_text)[:400]\n",
    "                print(f\"[error] collect failed for {day_iso}: {why or '(no details)'}\")\n",
    "                _write_day_summary(day_dir, {\n",
    "                    \"missing_count\": 0, \"excel_rows\": 0, \"json_rows\": 0,\n",
    "                    \"tracker_path\": \"\", \"export_clicked\": False, \"xlsx_path\": \"\",\n",
    "                    \"xlsx_size\": 0, \"headers_seen\": [], \"json_file_exists\": False,\n",
    "                    \"ok\": False, \"why\": why,\n",
    "                    \"endpoint\": \"fallback/collect-missing\", \"ts\": datetime.now().isoformat(),\n",
    "                })\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting\")\n",
    "                    _write_range_summary(BASE_DIR,\n",
    "                                         start_date=_iso(start_dt),\n",
    "                                         end_date=_iso(end_dt),\n",
    "                                         total_missing=total_missing,\n",
    "                                         days_processed=days_processed,\n",
    "                                         errors=errors)\n",
    "                    return\n",
    "\n",
    "        # summary\n",
    "        print(\"\\n[summary]\")\n",
    "        print(f\"  processed days  : {days_processed}\")\n",
    "        print(f\"  total missing   : {total_missing}\")\n",
    "        print(f\"  errors          : {errors}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=_iso(start_dt),\n",
    "                             end_date=_iso(end_dt),\n",
    "                             total_missing=total_missing,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "    else:\n",
    "        # Range call (server already iterates day-by-day)\n",
    "        df = _iso(start_dt)\n",
    "        dt = _iso(end_dt)\n",
    "        print(f\"[range] single call: {df}..{dt}\")\n",
    "        ok, resp_json, resp_text = _post_collect(df, dt)\n",
    "\n",
    "        if ok and isinstance(resp_json, dict) and resp_json.get(\"ok\"):\n",
    "            per_day = resp_json.get(\"per_day\") or []\n",
    "            days_processed = len([x for x in per_day if x.get(\"ok\")])\n",
    "            total_missing = sum(int(x.get(\"missing\", 0)) for x in per_day)\n",
    "            for found in per_day:\n",
    "                day_iso = found.get(\"date\") or \"\"\n",
    "                if not day_iso:\n",
    "                    continue\n",
    "                day_dir = os.path.join(BASE_DIR, day_iso)\n",
    "                _write_day_summary(day_dir, {\n",
    "                    \"missing_count\": found.get(\"missing\", 0),\n",
    "                    \"excel_rows\": found.get(\"excel_rows\", 0),\n",
    "                    \"json_rows\": found.get(\"json_rows\", 0),\n",
    "                    \"tracker_path\": found.get(\"tracker_path\", \"\"),\n",
    "                    \"export_clicked\": found.get(\"export_clicked\", False),\n",
    "                    \"xlsx_path\": found.get(\"xlsx_path\", \"\"),\n",
    "                    \"xlsx_size\": found.get(\"xlsx_size\", 0),\n",
    "                    \"headers_seen\": found.get(\"headers_seen\", []),\n",
    "                    \"json_file_exists\": found.get(\"json_file_exists\", False),\n",
    "                    \"ok\": found.get(\"ok\", False),\n",
    "                    \"why\": found.get(\"why\", \"\"),\n",
    "                    \"endpoint\": \"fallback/collect-missing\",\n",
    "                    \"ts\": datetime.now().isoformat(),\n",
    "                })\n",
    "        else:\n",
    "            errors = 1\n",
    "            why = \"\"\n",
    "            if resp_json and isinstance(resp_json, dict):\n",
    "                why = resp_json.get(\"error\") or resp_json.get(\"message\") or \"\"\n",
    "            elif resp_text:\n",
    "                why = str(resp_text)[:400]\n",
    "            print(f\"[error] range collect failed: {why or '(no details)'}\")\n",
    "\n",
    "        _write_range_summary(BASE_DIR,\n",
    "                             start_date=df,\n",
    "                             end_date=dt,\n",
    "                             total_missing=total_missing,\n",
    "                             days_processed=days_processed,\n",
    "                             errors=errors)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17c5de",
   "metadata": {},
   "source": [
    "docker exec -it 969e03517899 sh -lc 'mkdir -p /app/WebService/data/2025-10-05'\n",
    "\n",
    "docker cp .\\WebService\\data\\2025-10-05\\exchange_rates_payload.json 969e03517899:/app/WebService/data/2025-10-05\\exchange_rates_payload.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db8b2a",
   "metadata": {},
   "source": [
    "# New post_fallback_refill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f287f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': True, 'total_days': 1, 'posted_days': 1, 'skipped_days': 0, 'errors': 0, 'total_rows_sent': 155, 'per_day': [{'date': '2025-11-23', 'ok': True, 'input_rows': 155, 'kept_rows': 155, 'dedup_removed': 0, 'dropped_no_rate': 0, 'batch_id': 'fallback-refill-2025-11-23-0dbee720', 'duration_sec': 2414.74, 'reports': {'dir': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720', 'result_json': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720\\\\result.json', 'failed_json': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720\\\\failed.json', 'failed_csv': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720\\\\failed.csv', 'skipped_json': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720\\\\skipped.json', 'skipped_csv': 'reports\\\\2025-11-23\\\\fallback-refill-2025-11-23-0dbee720\\\\skipped.csv'}, 'records_day': '2025-11-23', 'live_has_pending': False, 'created': 155, 'failed': 0, 'skipped': 0}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "BASE = \"http://127.0.0.1:8000\"\n",
    "params = {\"date_from\": \"2025-11-23\", \"date_to\": \"2025-11-23\"}\n",
    "r = requests.post(f\"{BASE}/currency/exchange-rates/fallback/refill-missing\", params=params, timeout=12000)\n",
    "r.raise_for_status()\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2c83d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2529"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeaad35",
   "metadata": {},
   "source": [
    "# post_fallback_refill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaaba4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range] 2025-10-05 → 2025-10-05 (inclusive)\n",
      "[config] batch_size = 50, source = WebService/TrackDrivers/Fallback, endpoint = /batch\n",
      "\n",
      "\n",
      "=== DAY 2025-10-05 ===\n",
      "[skip] fallback file not found: WebService/TrackDrivers/Fallback\\2025-10-05.json\n",
      "[skip] no valid fallback rows for 2025-10-05\n",
      "\n",
      "[summary]\n",
      "  days in range         : 1\n",
      "  posted days           : 0\n",
      "  posted batches        : 0\n",
      "  rows sent             : 0\n",
      "  skipped days          : 1\n",
      "  errors                : 0\n",
      "  duplicates removed    : 0\n",
      "  same-currency dropped : 0\n"
     ]
    }
   ],
   "source": [
    "# post_fallback_refill.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "API_BASE = \"http://127.0.0.1:8000\"\n",
    "API_URL_BATCH = f\"{API_BASE}/currency/exchange-rates/batch\"  # non-streaming\n",
    "\n",
    "# Where the fallback collector saved the “missing” rows\n",
    "FALLBACK_DIR = \"WebService/TrackDrivers/Fallback\"\n",
    "\n",
    "# Accepts DD-MM-YYYY or YYYY-MM-DD (files are named YYYY-MM-DD.json)\n",
    "START_DATE = \"05-10-2025\"\n",
    "END_DATE   = \"05-10-2025\"\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# HTTP timeouts / retries\n",
    "REQUEST_TIMEOUT = 300  # seconds per HTTP request\n",
    "RETRY_MAX = 4\n",
    "RETRY_BACKOFF_BASE = 2  # seconds (exponential backoff base)\n",
    "\n",
    "# Behavior on failures\n",
    "STOP_ON_ERROR = False  # stop immediately if a batch fails\n",
    "\n",
    "# Input hygiene\n",
    "DROP_SAME_CURRENCY = True  # drop items where FromCurrency == ToCurrency\n",
    "ROUND_RATE_5DP = True      # coerce ExchangeRate to string with 5 dp (API model expectation)\n",
    "\n",
    "# Output (writes next to each <YYYY-MM-DD>.json in FALLBACK_DIR)\n",
    "WRITE_DAY_SUMMARY = True\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "_DD_MM_YYYY_DASH = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\")\n",
    "_YYYY_MM_DD = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\n",
    "\n",
    "def _parse_date_any(s: str) -> datetime:\n",
    "    s = (s or \"\").strip()\n",
    "    m = _DD_MM_YYYY_DASH.fullmatch(s)\n",
    "    if m:\n",
    "        dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    m = _YYYY_MM_DD.fullmatch(s)\n",
    "    if m:\n",
    "        yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "        return datetime(int(yyyy), int(mm), int(dd))\n",
    "    raise ValueError(f\"Date must be DD-MM-YYYY or YYYY-MM-DD, got: {s!r}\")\n",
    "\n",
    "def _daterange_inclusive(start_dt: datetime, end_dt: datetime):\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        yield cur\n",
    "        cur = cur + timedelta(days=1)\n",
    "\n",
    "def _chunked(lst: List[Any], size: int) -> List[List[Any]]:\n",
    "    size = max(1, int(size))\n",
    "    return [lst[i:i+size] for i in range(0, len(lst), size)]\n",
    "\n",
    "def _post_with_retries(url: str, *, json_body: Any, timeout: Optional[int]) -> requests.Response:\n",
    "    last_exc = None\n",
    "    session = requests.Session()\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = session.post(url, json=json_body, timeout=timeout, headers=headers)\n",
    "            if r.status_code in (429, 502, 503, 504):\n",
    "                raise requests.RequestException(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.ConnectTimeout, requests.ReadTimeout, requests.ConnectionError, requests.RequestException) as e:\n",
    "            last_exc = e\n",
    "            if attempt >= RETRY_MAX:\n",
    "                break\n",
    "            sleep_s = max(1, int(RETRY_BACKOFF_BASE) ** (attempt - 1))\n",
    "            print(f\"[retry] attempt {attempt}/{RETRY_MAX} failed: {e}. Backing off {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "    if isinstance(last_exc, Exception):\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"Unknown POST failure\")\n",
    "\n",
    "def _post_payload_batch(payload: List[Dict[str, Any]]) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:\n",
    "    try:\n",
    "        r = _post_with_retries(API_URL_BATCH, json_body=payload, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            return True, r.json(), None\n",
    "        except Exception:\n",
    "            return True, None, r.text\n",
    "    except Exception as e:\n",
    "        body = None\n",
    "        if isinstance(e, requests.RequestException) and getattr(e, \"response\", None) is not None:\n",
    "            try:\n",
    "                body = e.response.text\n",
    "            except Exception:\n",
    "                body = None\n",
    "        return False, None, f\"{e}\\n{('Response body: ' + body) if body else ''}\"\n",
    "\n",
    "# ---------- normalization used by API model (ExchangeRateItem) ----------\n",
    "def _q_norm(q: str | None) -> str:\n",
    "    s = (q or \"Direct\").strip().lower()\n",
    "    return \"Indirect\" if s.startswith(\"ind\") else \"Direct\"\n",
    "\n",
    "def _date_to_ddmmyyyy(s: str | None) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    fmts = [\n",
    "        \"%d.%m.%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\",\n",
    "        \"%Y/%m/%d\", \"%Y%m%d\", \"%Y-%d-%m\"\n",
    "    ]\n",
    "    for f in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(s, f)\n",
    "            return dt.strftime(\"%d.%m.%Y\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # If it already looks like DD.MM.YYYY but failed parse for some reason, keep as-is\n",
    "    return s\n",
    "\n",
    "def _rate_5dp(v: Any) -> str:\n",
    "    q = Decimal(str(v))\n",
    "    if q <= 0:\n",
    "        # Let API reject if invalid; still format\n",
    "        q = Decimal(\"0.00001\")\n",
    "    q = q.quantize(Decimal(\"0.00001\"), rounding=ROUND_HALF_UP)\n",
    "    return f\"{q:.5f}\"\n",
    "\n",
    "def _norm_row(r: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        return {\n",
    "            \"ExchangeRateType\": (r.get(\"ExchangeRateType\") or \"\").strip().upper(),\n",
    "            \"FromCurrency\": (r.get(\"FromCurrency\") or \"\").strip().upper(),\n",
    "            \"ToCurrency\": (r.get(\"ToCurrency\") or \"\").strip().upper(),\n",
    "            \"ValidFrom\": _date_to_ddmmyyyy(r.get(\"ValidFrom\")),\n",
    "            \"Quotation\": _q_norm(r.get(\"Quotation\")),\n",
    "            \"ExchangeRate\": _rate_5dp(r.get(\"ExchangeRate\")),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _dedupe_payload(payload: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:\n",
    "    \"\"\"\n",
    "    Deduplicate by logical key (Type, From, To, Date, Quotation). Preserve order.\n",
    "    \"\"\"\n",
    "    seen: set[Tuple[str, str, str, str, str]] = set()\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    removed = 0\n",
    "    for r in payload:\n",
    "        key = (\n",
    "            (r.get(\"ExchangeRateType\") or \"\").upper(),\n",
    "            (r.get(\"FromCurrency\") or \"\").upper(),\n",
    "            (r.get(\"ToCurrency\") or \"\").upper(),\n",
    "            (r.get(\"ValidFrom\") or \"\"),\n",
    "            _q_norm(r.get(\"Quotation\")),\n",
    "        )\n",
    "        if key in seen:\n",
    "            removed += 1\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(r)\n",
    "    return out, removed\n",
    "\n",
    "def _filter_same_currency(payload: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    dropped = 0\n",
    "    for r in payload:\n",
    "        f = (r.get(\"FromCurrency\") or \"\").upper()\n",
    "        t = (r.get(\"ToCurrency\") or \"\").upper()\n",
    "        if f and t and f == t and DROP_SAME_CURRENCY:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        out.append(r)\n",
    "    return out, dropped\n",
    "\n",
    "# ---------- IO ----------\n",
    "def _load_fallback_for_day(day_iso: str) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Read WebService/TrackDrivers/Fallback/<YYYY-MM-DD>.json\n",
    "    Return list[dict] or None if missing/bad.\n",
    "    \"\"\"\n",
    "    path = os.path.join(FALLBACK_DIR, f\"{day_iso}.json\")\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[skip] fallback file not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f) or []\n",
    "        if not isinstance(data, list):\n",
    "            print(f\"[warn] fallback file is not a list: {path}\")\n",
    "            return None\n",
    "        # Normalize rows to API model shape\n",
    "        norm: List[Dict[str, Any]] = []\n",
    "        for r in data:\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            nr = _norm_row(r)\n",
    "            if nr:\n",
    "                norm.append(nr)\n",
    "        return norm\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] failed reading fallback {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _write_day_summary(day_iso: str, *, total_rows: int, rows_after_filters: int,\n",
    "                       dupes_removed: int, same_currency_dropped: int,\n",
    "                       posted_batches: int, rows_sent: int, errors: int) -> None:\n",
    "    if not WRITE_DAY_SUMMARY:\n",
    "        return\n",
    "    try:\n",
    "        out = {\n",
    "            \"total_rows_in_fallback\": total_rows,\n",
    "            \"rows_after_filters\": rows_after_filters,\n",
    "            \"duplicates_removed\": dupes_removed,\n",
    "            \"same_currency_dropped\": same_currency_dropped,\n",
    "            \"posted_batches\": posted_batches,\n",
    "            \"rows_sent\": rows_sent,\n",
    "            \"errors\": errors,\n",
    "            \"endpoint\": \"/currency/exchange-rates/batch\",\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"ts\": datetime.now().isoformat(),\n",
    "        }\n",
    "        path = os.path.join(FALLBACK_DIR, f\"{day_iso}.refill_summary.json\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(out, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] write summary failed for {day_iso}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    start_dt = _parse_date_any(START_DATE)\n",
    "    end_dt   = _parse_date_any(END_DATE)\n",
    "    if end_dt < start_dt:\n",
    "        raise SystemExit(f\"END_DATE {END_DATE} is before START_DATE {START_DATE}\")\n",
    "\n",
    "    bs = max(1, int(BATCH_SIZE))\n",
    "\n",
    "    total_days = 0\n",
    "    posted_days = 0\n",
    "    posted_batches = 0\n",
    "    rows_sent = 0\n",
    "    skipped_days = 0\n",
    "    errors = 0\n",
    "    total_dupes_removed = 0\n",
    "    total_same_currency_dropped = 0\n",
    "\n",
    "    print(f\"[range] {start_dt.date()} → {end_dt.date()} (inclusive)\")\n",
    "    print(f\"[config] batch_size = {bs}, source = {FALLBACK_DIR}, endpoint = /batch\\n\")\n",
    "\n",
    "    for d in _daterange_inclusive(start_dt, end_dt):\n",
    "        total_days += 1\n",
    "        day_iso = d.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\n=== DAY {day_iso} ===\")\n",
    "\n",
    "        payload = _load_fallback_for_day(day_iso)\n",
    "        if not payload:\n",
    "            print(f\"[skip] no valid fallback rows for {day_iso}\")\n",
    "            skipped_days += 1\n",
    "            continue\n",
    "\n",
    "        total_rows_in_file = len(payload)\n",
    "\n",
    "        # Hygiene (dedupe, drop same-currency, already normalized date/quotation/rate)\n",
    "        deduped_payload, dupes_removed = _dedupe_payload(payload)\n",
    "        total_dupes_removed += dupes_removed\n",
    "\n",
    "        filtered_payload, same_drop = _filter_same_currency(deduped_payload)\n",
    "        total_same_currency_dropped += same_drop\n",
    "\n",
    "        n_after = len(filtered_payload)\n",
    "        print(f\"[day] rows: {total_rows_in_file} → {n_after} \"\n",
    "              f\"(dedup removed {dupes_removed}, same-currency dropped {same_drop})\")\n",
    "\n",
    "        if n_after == 0:\n",
    "            _write_day_summary(\n",
    "                day_iso,\n",
    "                total_rows=total_rows_in_file,\n",
    "                rows_after_filters=n_after,\n",
    "                dupes_removed=dupes_removed,\n",
    "                same_currency_dropped=same_drop,\n",
    "                posted_batches=0,\n",
    "                rows_sent=0,\n",
    "                errors=0,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        chunks = _chunked(filtered_payload, bs)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"[day] batching: {total_chunks} batch(es) of up to {bs}\")\n",
    "\n",
    "        day_had_success = False\n",
    "        day_errors = 0\n",
    "        day_rows_sent = 0\n",
    "        day_batches_posted = 0\n",
    "\n",
    "        for idx, batch in enumerate(chunks, start=1):\n",
    "            print(f\"[post] {day_iso} | batch {idx}/{total_chunks}: {len(batch)} rows\")\n",
    "            ok, resp_json, resp_text = _post_payload_batch(batch)\n",
    "            if ok:\n",
    "                day_had_success = True\n",
    "                day_batches_posted += 1\n",
    "                posted_batches += 1\n",
    "                day_rows_sent += len(batch)\n",
    "                rows_sent += len(batch)\n",
    "                # Optionally inspect resp_json here\n",
    "            else:\n",
    "                day_errors += 1\n",
    "                errors += 1\n",
    "                print(f\"[error] POST failed for {day_iso} batch {idx}/{total_chunks}:\\n\"\n",
    "                      f\"{(resp_text or '(no body)')[:1000]}\")\n",
    "                if STOP_ON_ERROR:\n",
    "                    print(\"[halt] STOP_ON_ERROR=True → halting at this batch\")\n",
    "                    _write_day_summary(\n",
    "                        day_iso,\n",
    "                        total_rows=total_rows_in_file,\n",
    "                        rows_after_filters=n_after,\n",
    "                        dupes_removed=dupes_removed,\n",
    "                        same_currency_dropped=same_drop,\n",
    "                        posted_batches=day_batches_posted,\n",
    "                        rows_sent=day_rows_sent,\n",
    "                        errors=day_errors,\n",
    "                    )\n",
    "                    # Final tallies before exit\n",
    "                    print(\"\\n[summary]\")\n",
    "                    print(f\"  days in range         : {total_days}\")\n",
    "                    print(f\"  posted days           : {posted_days}\")\n",
    "                    print(f\"  posted batches        : {posted_batches}\")\n",
    "                    print(f\"  rows sent             : {rows_sent}\")\n",
    "                    print(f\"  skipped days          : {skipped_days}\")\n",
    "                    print(f\"  errors                : {errors}\")\n",
    "                    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "                    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "                    return\n",
    "\n",
    "        if day_had_success:\n",
    "            posted_days += 1\n",
    "\n",
    "        _write_day_summary(\n",
    "            day_iso,\n",
    "            total_rows=total_rows_in_file,\n",
    "            rows_after_filters=n_after,\n",
    "            dupes_removed=dupes_removed,\n",
    "            same_currency_dropped=same_drop,\n",
    "            posted_batches=day_batches_posted,\n",
    "            rows_sent=day_rows_sent,\n",
    "            errors=day_errors,\n",
    "        )\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  days in range         : {total_days}\")\n",
    "    print(f\"  posted days           : {posted_days}\")\n",
    "    print(f\"  posted batches        : {posted_batches}\")\n",
    "    print(f\"  rows sent             : {rows_sent}\")\n",
    "    print(f\"  skipped days          : {skipped_days}\")\n",
    "    print(f\"  errors                : {errors}\")\n",
    "    print(f\"  duplicates removed    : {total_dupes_removed}\")\n",
    "    print(f\"  same-currency dropped : {total_same_currency_dropped}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
