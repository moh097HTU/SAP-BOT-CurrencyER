{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c06ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.15\n",
      "3.146\n"
     ]
    }
   ],
   "source": [
    "x = 3.14559\n",
    "print(round(x, 2))  \n",
    "print(round(x, 3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d181be5",
   "metadata": {},
   "source": [
    "# Ghanem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a61cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch] 2025-07-01 → WebService/data\\2025-07-01\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/01/2025', 'ToDate': '07/01/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-02 → WebService/data\\2025-07-02\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/02/2025', 'ToDate': '07/02/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-03 → WebService/data\\2025-07-03\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/03/2025', 'ToDate': '07/03/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-04 → WebService/data\\2025-07-04\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/04/2025', 'ToDate': '07/04/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-05 → WebService/data\\2025-07-05\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/05/2025', 'ToDate': '07/05/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-06 → WebService/data\\2025-07-06\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/06/2025', 'ToDate': '07/06/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-07 → WebService/data\\2025-07-07\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/07/2025', 'ToDate': '07/07/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-08 → WebService/data\\2025-07-08\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/08/2025', 'ToDate': '07/08/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-09 → WebService/data\\2025-07-09\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/09/2025', 'ToDate': '07/09/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-10 → WebService/data\\2025-07-10\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/10/2025', 'ToDate': '07/10/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-11 → WebService/data\\2025-07-11\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/11/2025', 'ToDate': '07/11/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-12 → WebService/data\\2025-07-12\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/12/2025', 'ToDate': '07/12/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-13 → WebService/data\\2025-07-13\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/13/2025', 'ToDate': '07/13/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-14 → WebService/data\\2025-07-14\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/14/2025', 'ToDate': '07/14/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-15 → WebService/data\\2025-07-15\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/15/2025', 'ToDate': '07/15/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-16 → WebService/data\\2025-07-16\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/16/2025', 'ToDate': '07/16/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-17 → WebService/data\\2025-07-17\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/17/2025', 'ToDate': '07/17/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-18 → WebService/data\\2025-07-18\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/18/2025', 'ToDate': '07/18/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-19 → WebService/data\\2025-07-19\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/19/2025', 'ToDate': '07/19/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-20 → WebService/data\\2025-07-20\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/20/2025', 'ToDate': '07/20/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-21 → WebService/data\\2025-07-21\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/21/2025', 'ToDate': '07/21/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-22 → WebService/data\\2025-07-22\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/22/2025', 'ToDate': '07/22/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-23 → WebService/data\\2025-07-23\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/23/2025', 'ToDate': '07/23/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-24 → WebService/data\\2025-07-24\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/24/2025', 'ToDate': '07/24/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-25 → WebService/data\\2025-07-25\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/25/2025', 'ToDate': '07/25/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-26 → WebService/data\\2025-07-26\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/26/2025', 'ToDate': '07/26/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-27 → WebService/data\\2025-07-27\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/27/2025', 'ToDate': '07/27/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-28 → WebService/data\\2025-07-28\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/28/2025', 'ToDate': '07/28/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-29 → WebService/data\\2025-07-29\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/29/2025', 'ToDate': '07/29/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-30 → WebService/data\\2025-07-30\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/30/2025', 'ToDate': '07/30/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[fetch] 2025-07-31 → WebService/data\\2025-07-31\n",
      "→ POST http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates body={'FromDate': '07/31/2025', 'ToDate': '07/31/2025'} (attempt 1/3)\n",
      "   HTTP 200\n",
      "[done] fetch_exchange_range finished.\n"
     ]
    }
   ],
   "source": [
    "# fetch_exchange_range.py\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import socket\n",
    "from datetime import date, timedelta\n",
    "from typing import Iterable, Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import ConnectTimeout, ReadTimeout, ProxyError, SSLError, ConnectionError as ReqConnectionError\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_URL = \"http://192.168.0.2/Production_ExchangeRatesAPI/ExchangeRates/getExchangeRates\"\n",
    "BASE_SAVE_DIR = \"WebService/data\"\n",
    "\n",
    "# Timeouts: (connect_timeout, read_timeout)\n",
    "CONNECT_TIMEOUT = 5\n",
    "READ_TIMEOUT = 30\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF_BASE = 2\n",
    "\n",
    "DATE_RE_MMDDYYYY = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "NO_PROXY = {\"http\": None, \"https\": None}\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def validate_mmddyyyy(s: str) -> None:\n",
    "    if not DATE_RE_MMDDYYYY.match(s or \"\"):\n",
    "        raise ValueError(f\"Date must be MM/DD/YYYY, got {s}\")\n",
    "\n",
    "def mmddyyyy(d: date) -> str:\n",
    "    return f\"{d.month:02d}/{d.day:02d}/{d.year:04d}\"\n",
    "\n",
    "def daterange_days(start: date, end: date) -> Iterable[date]:\n",
    "    d = start\n",
    "    while d <= end:\n",
    "        yield d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _tcp_preflight(host: str, port: int = 80, timeout: int = 3) -> bool:\n",
    "    \"\"\"Quick TCP connect preflight—returns True if TCP handshake succeeds.\"\"\"\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=timeout):\n",
    "            return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def _make_session(disable_env_proxies: bool = True, force_no_proxy: bool = True) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.trust_env = not disable_env_proxies  # False → do NOT inherit env proxies\n",
    "    if force_no_proxy:\n",
    "        s.proxies.update(NO_PROXY)\n",
    "    s.headers.update({\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"})\n",
    "    return s\n",
    "\n",
    "# ---------- shaping ----------\n",
    "def flatten_rows(obj: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Normalize unknown API shape to a list[dict] for saving CSV/JSON.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        return [x if isinstance(x, dict) else {\"value\": x} for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        if isinstance(obj.get(\"data\"), list):\n",
    "            return [x if isinstance(x, dict) else {\"value\": x} for x in obj[\"data\"]]\n",
    "        return [obj]\n",
    "    return [{\"value\": obj}]\n",
    "\n",
    "# ---------- filters (hard) ----------\n",
    "_FRF = \"FRF\"\n",
    "\n",
    "def _extract_pair_ci(row: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    if not isinstance(row, dict):\n",
    "        return \"\", \"\"\n",
    "    lm = {k.lower(): k for k in row.keys()}\n",
    "\n",
    "    def _pick(keys: List[str]) -> str:\n",
    "        for k in keys:\n",
    "            real = lm.get(k)\n",
    "            if real:\n",
    "                v = row.get(real, \"\")\n",
    "                return str(v).strip().upper() if isinstance(v, (str, int, float)) else \"\"\n",
    "        return \"\"\n",
    "\n",
    "    f = _pick([\"fromcurrency\", \"sourcecurrency\", \"from\", \"basecurrency\"])\n",
    "    t = _pick([\"tocurrency\", \"targetcurrency\", \"to\", \"quotecurrency\", \"targetcurrency\"])\n",
    "    return f, t\n",
    "\n",
    "def _same_currency_pair(row: Dict[str, Any]) -> bool:\n",
    "    f, t = _extract_pair_ci(row)\n",
    "    return bool(f) and f == t\n",
    "\n",
    "def _has_frf(row: Dict[str, Any]) -> bool:\n",
    "    f, t = _extract_pair_ci(row)\n",
    "    return f == _FRF or t == _FRF\n",
    "\n",
    "def _filter_rows_hard(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Remove rows where From==To or either side is FRF.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        if _same_currency_pair(r):\n",
    "            continue\n",
    "        if _has_frf(r):\n",
    "            continue\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "# ---------- IO ----------\n",
    "def _save_day_json(out_dir: str, d: date, rows: List[Dict[str, Any]]) -> str:\n",
    "    json_dir = os.path.join(out_dir, \"exchange_rates_json\")\n",
    "    ensure_dir(json_dir)\n",
    "    p = os.path.join(json_dir, f\"{d.isoformat()}.json\")\n",
    "    # Save under {\"data\": [...] } like your Fixer script\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"data\": rows}, f, ensure_ascii=False, indent=2)\n",
    "    return p\n",
    "\n",
    "def _append_csv(out_dir: str, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows:\n",
    "        return\n",
    "    csv_path = os.path.join(out_dir, \"exchange_rates_agg.csv\")\n",
    "\n",
    "    # discover headers from existing + new\n",
    "    all_keys = set()\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rdr = csv.DictReader(f)\n",
    "            all_keys.update(rdr.fieldnames or [])\n",
    "    for r in rows:\n",
    "        all_keys.update(r.keys())\n",
    "    fieldnames = sorted(all_keys)\n",
    "\n",
    "    # load existing\n",
    "    existing: List[Dict[str, Any]] = []\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rdr = csv.DictReader(f)\n",
    "            existing = list(rdr)\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in existing:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "# ---------- HTTP ----------\n",
    "def _post_one_day(d: date, session: requests.Session) -> Any:\n",
    "    payload = {\"FromDate\": mmddyyyy(d), \"ToDate\": mmddyyyy(d)}\n",
    "    validate_mmddyyyy(payload[\"FromDate\"]); validate_mmddyyyy(payload[\"ToDate\"])\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f\"→ POST {BASE_URL} body={payload} (attempt {attempt}/{MAX_RETRIES})\")\n",
    "            r = session.post(\n",
    "                BASE_URL,\n",
    "                json=payload,\n",
    "                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT),\n",
    "                allow_redirects=False,\n",
    "            )\n",
    "            print(f\"   HTTP {r.status_code}\")\n",
    "            r.raise_for_status()\n",
    "            try:\n",
    "                return r.json()\n",
    "            except ValueError:\n",
    "                return {\"raw\": r.text}\n",
    "        except (ConnectTimeout, ReadTimeout) as e:\n",
    "            last_err = e\n",
    "            print(f\"   Timeout: {e}\")\n",
    "        except (ProxyError, SSLError, ReqConnectionError, requests.HTTPError) as e:\n",
    "            last_err = e\n",
    "            print(f\"   Connection/HTTP error: {e}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"   Other error: {e}\")\n",
    "\n",
    "        time.sleep(min(RETRY_BACKOFF_BASE ** attempt, 8))\n",
    "\n",
    "    raise RuntimeError(f\"[{d}] Failed after {MAX_RETRIES} attempts: {last_err}\")\n",
    "\n",
    "# ---------- main ----------\n",
    "def fetch_exchange_range(start: date, end: date) -> None:\n",
    "    # Preflight\n",
    "    host = BASE_URL.split(\"://\", 1)[1].split(\"/\", 1)[0]\n",
    "    if not _tcp_preflight(host, 80, timeout=3):\n",
    "        raise RuntimeError(\n",
    "            f\"Cannot open TCP to {host}:80. Likely VPN route/firewall issue. \"\n",
    "            \"Check that 192.168.0.0/24 is routed via VPN and outbound :80 is allowed.\"\n",
    "        )\n",
    "\n",
    "    session = _make_session(disable_env_proxies=True, force_no_proxy=True)\n",
    "\n",
    "    for d in daterange_days(start, end):\n",
    "        day_dir = os.path.join(BASE_SAVE_DIR, d.isoformat())\n",
    "        ensure_dir(day_dir)\n",
    "        print(f\"[fetch] {d.isoformat()} → {day_dir}\")\n",
    "\n",
    "        obj = _post_one_day(d, session)\n",
    "\n",
    "        # Normalize to list[dict] and apply hard filters\n",
    "        rows = flatten_rows(obj)\n",
    "        rows = _filter_rows_hard(rows)\n",
    "\n",
    "        # Optional stamp (helps later if CSV is used as source)\n",
    "        for r in rows:\n",
    "            if isinstance(r, dict):\n",
    "                r.setdefault(\"_service_date\", d.isoformat())\n",
    "\n",
    "        _save_day_json(day_dir, d, rows)\n",
    "        _append_csv(day_dir, rows)\n",
    "\n",
    "    print(\"[done] fetch_exchange_range finished.\")\n",
    "\n",
    "# === RUN EXAMPLE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # set your desired window here:\n",
    "    fetch_exchange_range(date(2025, 7, 1), date(2025, 7, 31))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2991b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[needed] 208 pairs loaded from WebService/needed_rates.txt\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-01\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-01\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-01\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-02\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-02\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-02\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-03\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-03\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-03\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-04\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-04\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-04\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-05\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-05\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-05\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-06\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-06\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-06\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-07\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-07\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-07\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-08\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-08\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-08\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-09\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-09\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-09\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-10\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-10\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-10\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-11\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-11\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-11\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-12\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-12\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-12\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-13\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-13\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-13\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-14\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-14\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-14\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-15\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-15\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-15\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-16\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-16\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-16\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-17\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-17\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-17\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-18\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-18\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-18\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-19\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-19\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-19\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-20\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-20\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-20\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-21\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-21\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-21\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-22\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-22\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-22\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-23\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-23\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-23\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-24\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-24\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-24\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-25\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-25\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-25\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-26\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-26\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-26\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-27\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-27\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-27\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-28\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-28\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-28\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-29\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-29\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-29\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-30\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-30\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-30\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[run-dir] WebService/data\\2025-07-31\n",
      "[done] JSON files processed=1, written=1 → WebService/data\\2025-07-31\\exchange_rates_json_filtered\n",
      "[done] CSV rows in=784, out=199 → WebService/data\\2025-07-31\\exchange_rates_agg.filtered.csv\n",
      "\n",
      "[summary] JSON in=31, JSON out=31; CSV in=24304, CSV out=6169\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from typing import Dict, Any, List, Tuple, Set\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "\n",
    "# ✱ FIX: point to WebService/needed_rates.txt, not WebService/data/needed_rates.txt\n",
    "# You can override with env NEEDED_RATES_FILE if you want a different path.\n",
    "NEEDED_FILE = os.environ.get(\"NEEDED_RATES_FILE\", \"WebService/needed_rates.txt\")\n",
    "\n",
    "# match single-day directories: YYYY-MM-DD\n",
    "RUN_DIR_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _norm_currency(s: str) -> str:\n",
    "    return (s or \"\").strip().upper()\n",
    "\n",
    "def _find_key_case_insensitive(d: Dict[str, Any], *candidates: str) -> str:\n",
    "    lower_map = {k.lower(): k for k in d.keys()}\n",
    "    for c in candidates:\n",
    "        k = lower_map.get(c.lower())\n",
    "        if k is not None:\n",
    "            return k\n",
    "    return \"\"\n",
    "\n",
    "def _row_pair(row: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    if not isinstance(row, dict):\n",
    "        return \"\", \"\"\n",
    "    from_k = _find_key_case_insensitive(row, \"FromCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    to_k   = _find_key_case_insensitive(row, \"ToCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    f = _norm_currency(row.get(from_k, \"\")) if from_k else \"\"\n",
    "    t = _norm_currency(row.get(to_k, \"\"))   if to_k else \"\"\n",
    "    return f, t\n",
    "\n",
    "def load_needed_pairs(path: str) -> Set[Tuple[str, str]]:\n",
    "    pairs: Set[Tuple[str, str]] = set()\n",
    "    if not os.path.isfile(path):\n",
    "        raise SystemExit(\n",
    "            f\"[FATAL] needed list not found: {path}\\n\"\n",
    "            \"Create it (one pair per line like 'AED/USD') or set NEEDED_RATES_FILE env var.\"\n",
    "        )\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = s.split(\"/\") if \"/\" in s else s.replace(\" \", \"/\").split(\"/\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            a, b = _norm_currency(parts[0]), _norm_currency(parts[1])\n",
    "            if a and b:\n",
    "                pairs.add((a, b))\n",
    "    if not pairs:\n",
    "        raise SystemExit(f\"[FATAL] needed list is empty: {path}\")\n",
    "    print(f\"[needed] {len(pairs)} pairs loaded from {path}\")\n",
    "    return pairs\n",
    "\n",
    "def should_keep(row: Dict[str, Any], include_set: Set[Tuple[str, str]]) -> bool:\n",
    "    # include_set is guaranteed non-empty (see load_needed_pairs)\n",
    "    f, t = _row_pair(row)\n",
    "    if not f or not t:\n",
    "        return False\n",
    "    return (f, t) in include_set\n",
    "\n",
    "def filter_json_object(obj: Any, include_set: Set[Tuple[str, str]]) -> Any:\n",
    "    try:\n",
    "        if isinstance(obj, list):\n",
    "            return [r for r in obj if isinstance(r, dict) and should_keep(r, include_set)]\n",
    "        if isinstance(obj, dict):\n",
    "            if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "                new_obj = dict(obj)\n",
    "                new_obj[\"data\"] = [r for r in obj[\"data\"] if isinstance(r, dict) and should_keep(r, include_set)]\n",
    "                return new_obj\n",
    "            return {\"data\": [obj]} if should_keep(obj, include_set) else {\"data\": []}\n",
    "        return {\"data\": []}\n",
    "    except Exception:\n",
    "        return {\"data\": []}\n",
    "\n",
    "def filter_all_jsons(run_dir: str, include_set: Set[Tuple[str, str]]) -> Tuple[int, int]:\n",
    "    json_in = os.path.join(run_dir, \"exchange_rates_json\")\n",
    "    json_out = os.path.join(run_dir, \"exchange_rates_json_filtered\")\n",
    "    ensure_dir(json_out)\n",
    "\n",
    "    processed = 0\n",
    "    written = 0\n",
    "\n",
    "    if not os.path.isdir(json_in):\n",
    "        print(f\"[warn] JSON input dir not found: {json_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    for name in os.listdir(json_in):\n",
    "        if not name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        inp = os.path.join(json_in, name)\n",
    "        out = os.path.join(json_out, name)\n",
    "        try:\n",
    "            with open(inp, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "            new_obj = filter_json_object(obj, include_set)\n",
    "            with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(new_obj, f, ensure_ascii=False, indent=2)\n",
    "            processed += 1\n",
    "            written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[json] skip {inp}: {e}\")\n",
    "            processed += 1\n",
    "    return processed, written\n",
    "\n",
    "def filter_csv(run_dir: str, include_set: Set[Tuple[str, str]]) -> Tuple[int, int]:\n",
    "    csv_in = os.path.join(run_dir, \"exchange_rates_agg.csv\")\n",
    "    csv_out = os.path.join(run_dir, \"exchange_rates_agg.filtered.csv\")\n",
    "\n",
    "    if not os.path.isfile(csv_in):\n",
    "        print(f\"[warn] CSV input not found: {csv_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    with open(csv_in, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        fieldnames = rdr.fieldnames or []\n",
    "        rows = list(rdr)\n",
    "\n",
    "    if not fieldnames:\n",
    "        print(f\"[warn] CSV has no header: {csv_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    kept: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        if should_keep(r, include_set):\n",
    "            kept.append(r)\n",
    "\n",
    "    if \"FromCurrency\" not in fieldnames:\n",
    "        fieldnames = fieldnames + [\"FromCurrency\"]\n",
    "    if \"ToCurrency\" not in fieldnames:\n",
    "        fieldnames = fieldnames + [\"ToCurrency\"]\n",
    "\n",
    "    with open(csv_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in kept:\n",
    "            out_row = {k: r.get(k, \"\") for k in fieldnames}\n",
    "            fcur, tcur = _row_pair(r)\n",
    "            if fcur and not out_row.get(\"FromCurrency\"):\n",
    "                out_row[\"FromCurrency\"] = fcur\n",
    "            if tcur and not out_row.get(\"ToCurrency\"):\n",
    "                out_row[\"ToCurrency\"] = tcur\n",
    "            w.writerow(out_row)\n",
    "\n",
    "    return (len(rows), len(kept))\n",
    "\n",
    "def list_all_day_dirs(base_dir: str) -> List[str]:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        p = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(p) and RUN_DIR_PATTERN.match(name):\n",
    "            out.append(p)\n",
    "    out.sort()\n",
    "    return out\n",
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    day_dirs = list_all_day_dirs(BASE_DIR)\n",
    "    if not day_dirs:\n",
    "        raise SystemExit(f\"No day directories found under {BASE_DIR} matching YYYY-MM-DD\")\n",
    "\n",
    "    needed = load_needed_pairs(NEEDED_FILE)  # ✱ now fatal if missing/empty\n",
    "    total_json_in = total_json_out = total_csv_in = total_csv_out = 0\n",
    "\n",
    "    for run_dir in day_dirs:\n",
    "        print(f\"\\n[run-dir] {run_dir}\")\n",
    "        j_in, j_out = filter_all_jsons(run_dir, needed)\n",
    "        print(f\"[done] JSON files processed={j_in}, written={j_out} → {os.path.join(run_dir, 'exchange_rates_json_filtered')}\")\n",
    "        c_in, c_out = filter_csv(run_dir, needed)\n",
    "        print(f\"[done] CSV rows in={c_in}, out={c_out} → {os.path.join(run_dir, 'exchange_rates_agg.filtered.csv')}\")\n",
    "        total_json_in += j_in; total_json_out += j_out\n",
    "        total_csv_in  += c_in; total_csv_out  += c_out\n",
    "\n",
    "    print(f\"\\n[summary] JSON in={total_json_in}, JSON out={total_json_out}; CSV in={total_csv_in}, CSV out={total_csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7404b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rounding] loaded 31 currency rules from WebService/Currency.csv\n",
      "[run-dir] WebService/data\\2025-07-01\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-01\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-01\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-02\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-02\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-02\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-03\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-03\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-03\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-04\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-04\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-04\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-05\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-05\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-05\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-06\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-06\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-06\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-07\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-07\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-07\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-08\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-08\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-08\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-09\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-09\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-09\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-10\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-10\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-10\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-11\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-11\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-11\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-12\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-12\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-12\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-13\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-13\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-13\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-14\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-14\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-14\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-15\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-15\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-15\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-16\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-16\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-16\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-17\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-17\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-17\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-18\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-18\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-18\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-19\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-19\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-19\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-20\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-20\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-20\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-21\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-21\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-21\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-22\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-22\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-22\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-23\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-23\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-23\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-24\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-24\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-24\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-25\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-25\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-25\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-26\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-26\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-26\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-27\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-27\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-27\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-28\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-28\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-28\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-29\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-29\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-29\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-30\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-30\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-30\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-31\n",
      "[rounding] zero-exceptions saved: 18 → WebService/data\\2025-07-31\\zero_exception.json\n",
      "[source] CSV(filtered), rows=398; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-31\\exchange_rates_payload.json\n",
      "[summary] payloads written: 31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from decimal import Decimal, ROUND_HALF_UP, InvalidOperation\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "CURRENCY_RULES_CSV = \"WebService/Currency.csv\"  # round_digits,target_currency\n",
    "RUN_DIR_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "FNAME_DATE_RE = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})\\.json$\", re.IGNORECASE)\n",
    "\n",
    "def list_all_day_dirs(base_dir: str) -> List[str]:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        p = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(p) and RUN_DIR_PATTERN.match(name):\n",
    "            out.append(p)\n",
    "    out.sort()\n",
    "    return out\n",
    "\n",
    "def _norm_ccy(x: Any) -> str:\n",
    "    s = (x or \"\").strip().upper()\n",
    "    return s if re.fullmatch(r\"[A-Z]{3,4}\", s) else \"\"\n",
    "\n",
    "def _parse_rate(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    sx = str(x).strip().replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(sx)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _is_mmddyyyy(s: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{2}/\\d{2}/\\d{4}\", s or \"\"))\n",
    "\n",
    "def _to_ddmmyyyy_from_iso(iso: str) -> str:\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", iso or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _mmddyyyy_to_ddmmyyyy_dots(s: str) -> str:\n",
    "    m = re.fullmatch(r\"(\\d{2})/(\\d{2})/(\\d{4})\", s or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    mm, dd, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _key_ci(d: Dict[str, Any], *cands: str) -> Optional[str]:\n",
    "    lm = {k.lower(): k for k in d.keys()}\n",
    "    for c in cands:\n",
    "        k = lm.get(c.lower())\n",
    "        if k is not None:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def load_rounding_rules(csv_path: str) -> Dict[str, int]:\n",
    "    rules: Dict[str, int] = {}\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[warn] rounding rules CSV not found: {csv_path} (rates will not be rounded)\")\n",
    "        return rules\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            if not isinstance(row, dict):\n",
    "                continue\n",
    "            lower_map = {k.lower(): k for k in row.keys()}\n",
    "            k_digits = lower_map.get(\"round_digits\")\n",
    "            k_ccy    = lower_map.get(\"target_currency\")\n",
    "            if not k_digits or not k_ccy:\n",
    "                continue\n",
    "            try:\n",
    "                digits = int(str(row[k_digits]).strip())\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "            ccy = _norm_ccy(row[k_ccy])\n",
    "            if not ccy:\n",
    "                continue\n",
    "            if digits < 0: digits = 0\n",
    "            if digits > 12: digits = 12\n",
    "            rules[ccy] = digits\n",
    "    return rules\n",
    "\n",
    "def _round_rate_half_up(rate: float, digits: int) -> float:\n",
    "    try:\n",
    "        q = Decimal(\"1\").scaleb(-digits)\n",
    "        d = Decimal(str(rate)).quantize(q, rounding=ROUND_HALF_UP)\n",
    "        return float(d)\n",
    "    except (InvalidOperation, ValueError):\n",
    "        return rate\n",
    "\n",
    "def apply_rounding(rows: List[Dict[str, Any]], rules: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Apply rounding per rules to rows *in-place*.\n",
    "    NEW: If rounding would turn a non-zero rate into 0.0, keep the original rate\n",
    "    and record the case in a zero-exception list (returned).\n",
    "    Returns:\n",
    "        zero_exceptions: list of dicts describing rows where rounding would yield zero.\n",
    "    \"\"\"\n",
    "    zero_exceptions: List[Dict[str, Any]] = []\n",
    "    if not rules:\n",
    "        return zero_exceptions\n",
    "\n",
    "    for r in rows:\n",
    "        try:\n",
    "            to_ccy = _norm_ccy(r.get(\"ToCurrency\"))\n",
    "            if not to_ccy:\n",
    "                continue\n",
    "            digits = rules.get(to_ccy)\n",
    "            if digits is None:\n",
    "                continue\n",
    "            rate = r.get(\"ExchangeRate\")\n",
    "            if rate is None:\n",
    "                continue\n",
    "            rate_f = _parse_rate(rate)\n",
    "            if rate_f is None:\n",
    "                continue\n",
    "\n",
    "            rounded = _round_rate_half_up(rate_f, digits)\n",
    "\n",
    "            # If rounding makes a non-zero rate become zero, DON'T round.\n",
    "            if rate_f != 0.0 and rounded == 0.0:\n",
    "                zero_exceptions.append({\n",
    "                    \"FromCurrency\": r.get(\"FromCurrency\"),\n",
    "                    \"ToCurrency\": r.get(\"ToCurrency\"),\n",
    "                    \"ValidFrom\": r.get(\"ValidFrom\"),\n",
    "                    \"ExchangeRateType\": r.get(\"ExchangeRateType\"),\n",
    "                    \"Quotation\": r.get(\"Quotation\"),\n",
    "                    \"original_exchange_rate\": rate_f,\n",
    "                    \"attempted_digits\": digits,\n",
    "                    \"would_round_to\": rounded\n",
    "                })\n",
    "                # keep original: do NOT assign rounded\n",
    "                continue\n",
    "\n",
    "            # Normal case: apply rounded value\n",
    "            r[\"ExchangeRate\"] = rounded\n",
    "\n",
    "        except Exception:\n",
    "            # Swallow per-row errors to avoid breaking the whole day; no logging change here.\n",
    "            continue\n",
    "\n",
    "    return zero_exceptions\n",
    "\n",
    "def normalize_row_from_csv(raw: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    k_date_csv = _key_ci(raw, \"exChangeRateDate\")\n",
    "    valid_from = \"\"\n",
    "    if k_date_csv:\n",
    "        cand = (raw.get(k_date_csv) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    if not valid_from:\n",
    "        k_iso = _key_ci(raw, \"_service_date\")\n",
    "        if k_iso and raw.get(k_iso):\n",
    "            valid_from = _to_ddmmyyyy_from_iso(str(raw[k_iso]).strip())\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": \"M\",\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,\n",
    "        \"Quotation\": \"Direct\",\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "def normalize_row_from_json(raw: Dict[str, Any], fallback_iso_date: str = \"\") -> Optional[Dict[str, Any]]:\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    k_mmdd = _key_ci(raw, \"exChangeRateDate\")\n",
    "    k_other= _key_ci(raw, \"ValidFrom\", \"Date\", \"RateDate\", \"_service_date\")\n",
    "    valid_from = \"\"\n",
    "    if k_mmdd:\n",
    "        cand = (raw.get(k_mmdd) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    if not valid_from and k_other and raw.get(k_other):\n",
    "        s = str(raw[k_other]).strip()\n",
    "        if _is_mmddyyyy(s):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(s)\n",
    "        else:\n",
    "            valid_from = _to_ddmmyyyy_from_iso(s)\n",
    "\n",
    "    if not valid_from and fallback_iso_date:\n",
    "        valid_from = _to_ddmmyyyy_from_iso(fallback_iso_date)\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    k_type = _key_ci(raw, \"ExchangeRateType\", \"Type\")\n",
    "    k_quo  = _key_ci(raw, \"Quotation\", \"QuoteType\", \"Side\")\n",
    "    etype = str(raw.get(k_type)).strip() if (k_type and raw.get(k_type)) else \"M\"\n",
    "    quo   = str(raw.get(k_quo)).strip()  if (k_quo  and raw.get(k_quo))  else \"Direct\"\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": etype,\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,\n",
    "        \"Quotation\": quo,\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "def load_from_filtered_csv(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    path = os.path.join(run_dir, \"exchange_rates_agg.filtered.csv\")\n",
    "    if not os.path.isfile(path):\n",
    "        return []\n",
    "    with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        rows_raw = list(rdr)\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows_raw:\n",
    "        nr = normalize_row_from_csv(r)\n",
    "        if nr:\n",
    "            out.append(nr)\n",
    "        else:\n",
    "            skip_stats[\"csv_skipped\"] += 1\n",
    "    return out\n",
    "\n",
    "def load_from_filtered_jsons(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    base = os.path.join(run_dir, \"exchange_rates_json_filtered\")\n",
    "    if not os.path.isdir(base):\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for name in os.listdir(base):\n",
    "        if not name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        p = os.path.join(base, name)\n",
    "        fallback_iso = \"\"\n",
    "        m = FNAME_DATE_RE.match(name)\n",
    "        if m:\n",
    "            yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "            fallback_iso = f\"{yyyy}-{mm}-{dd}\"\n",
    "\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception:\n",
    "            skip_stats[\"bad_json\"] += 1\n",
    "            continue\n",
    "\n",
    "        if isinstance(obj, dict) and isinstance(obj.get(\"data\"), list):\n",
    "            iterable = obj[\"data\"]\n",
    "        elif isinstance(obj, list):\n",
    "            iterable = obj\n",
    "        elif isinstance(obj, dict):\n",
    "            iterable = [obj]\n",
    "        else:\n",
    "            skip_stats[\"unknown_shape\"] += 1\n",
    "            continue\n",
    "\n",
    "        for item in iterable:\n",
    "            if isinstance(item, dict):\n",
    "                nr = normalize_row_from_json(item, fallback_iso_date=fallback_iso)\n",
    "                if nr:\n",
    "                    out.append(nr)\n",
    "                else:\n",
    "                    skip_stats[\"json_row_skipped\"] += 1\n",
    "            else:\n",
    "                skip_stats[\"non_dict_row\"] += 1\n",
    "    return out\n",
    "\n",
    "def dedupe_sort(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen: Dict[Tuple[str, str, str, str, str], Dict[str, Any]] = {}\n",
    "    for r in rows:\n",
    "        key = (r[\"FromCurrency\"], r[\"ToCurrency\"], r[\"ValidFrom\"], r[\"ExchangeRateType\"], r[\"Quotation\"])\n",
    "        seen[key] = r\n",
    "    uniq = list(seen.values())\n",
    "\n",
    "    def _key(rw: Dict[str, Any]) -> Tuple[str, str, str]:\n",
    "        m = re.fullmatch(r\"(\\d{2})\\.(\\d{2})\\.(\\d{4})\", rw[\"ValidFrom\"])\n",
    "        ymd = f\"{m.group(3)}{m.group(2)}{m.group(1)}\" if m else \"00000000\"\n",
    "        return (ymd, rw[\"FromCurrency\"], rw[\"ToCurrency\"])\n",
    "\n",
    "    uniq.sort(key=_key)\n",
    "    return uniq\n",
    "\n",
    "# --- NEW: duplicate each logical row into M and P variants (before rounding) ---\n",
    "def _duplicate_rows_M_and_P(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        base = {\n",
    "            \"FromCurrency\": r.get(\"FromCurrency\"),\n",
    "            \"ToCurrency\": r.get(\"ToCurrency\"),\n",
    "            \"ValidFrom\": r.get(\"ValidFrom\"),\n",
    "            \"Quotation\": r.get(\"Quotation\"),\n",
    "            \"ExchangeRate\": r.get(\"ExchangeRate\"),\n",
    "        }\n",
    "        m_obj = dict(base)\n",
    "        m_obj[\"ExchangeRateType\"] = \"M\"\n",
    "        p_obj = dict(base)\n",
    "        p_obj[\"ExchangeRateType\"] = \"P\"\n",
    "        out.append(m_obj)\n",
    "        out.append(p_obj)\n",
    "    return out\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def build_payloads_for_all_days() -> List[str]:\n",
    "    written_paths: List[str] = []\n",
    "    day_dirs = list_all_day_dirs(BASE_DIR)\n",
    "    if not day_dirs:\n",
    "        print(f\"No day directories under {BASE_DIR} matching YYYY-MM-DD\")\n",
    "        return written_paths\n",
    "\n",
    "    rounding_rules = load_rounding_rules(CURRENCY_RULES_CSV)\n",
    "    if rounding_rules:\n",
    "        print(f\"[rounding] loaded {len(rounding_rules)} currency rules from {CURRENCY_RULES_CSV}\")\n",
    "    else:\n",
    "        print(f\"[rounding] no rules found (rates will not be rounded)\")\n",
    "\n",
    "    for run_dir in day_dirs:\n",
    "        print(f\"[run-dir] {run_dir}\")\n",
    "        skip_stats = {\n",
    "            \"csv_skipped\": 0,\n",
    "            \"bad_json\": 0,\n",
    "            \"unknown_shape\": 0,\n",
    "            \"json_row_skipped\": 0,\n",
    "            \"non_dict_row\": 0,\n",
    "        }\n",
    "\n",
    "        rows = load_from_filtered_csv(run_dir, skip_stats)\n",
    "        source = \"CSV(filtered)\"\n",
    "        if not rows:\n",
    "            rows = load_from_filtered_jsons(run_dir, skip_stats)\n",
    "            source = \"JSON(filtered)\"\n",
    "\n",
    "        # ✱ Fail fast if neither filtered source is present.\n",
    "        if not rows:\n",
    "            raise SystemExit(\n",
    "                f\"[FATAL] No filtered inputs found in {run_dir}.\\n\"\n",
    "                f\"Expected one of:\\n\"\n",
    "                f\"  - {os.path.join(run_dir, 'exchange_rates_agg.filtered.csv')}\\n\"\n",
    "                f\"  - {os.path.join(run_dir, 'exchange_rates_json_filtered')}/<*.json>\\n\"\n",
    "                f\"Run filter_to_needed_pairs.py AFTER fetch and BEFORE build.\"\n",
    "            )\n",
    "\n",
    "        # Duplicate every logical row into both M and P BEFORE rounding\n",
    "        rows = _duplicate_rows_M_and_P(rows)\n",
    "\n",
    "        # Apply rounding with zero-exception safeguard (covers both M and P)\n",
    "        zero_exceptions = apply_rounding(rows, rounding_rules)\n",
    "\n",
    "        # If any rounding would have produced zero, persist them to zero_exception.json\n",
    "        if zero_exceptions:\n",
    "            zp = os.path.join(run_dir, \"zero_exception.json\")\n",
    "            try:\n",
    "                with open(zp, \"w\", encoding=\"utf-8\") as zf:\n",
    "                    json.dump(zero_exceptions, zf, ensure_ascii=False, indent=2)\n",
    "                print(f\"[rounding] zero-exceptions saved: {len(zero_exceptions)} → {zp}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"[warn] failed to write zero_exception.json in {run_dir}: {ex}\")\n",
    "        else:\n",
    "            print(f\"[rounding] zero-exceptions: 0\")\n",
    "\n",
    "        rows = dedupe_sort(rows)\n",
    "        print(f\"[source] {source}, rows={len(rows)}; skips={skip_stats}\")\n",
    "\n",
    "        out_path = os.path.join(run_dir, \"exchange_rates_payload.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[written] {out_path}\")\n",
    "        written_paths.append(out_path)\n",
    "\n",
    "    print(f\"[summary] payloads written: {len(written_paths)}\")\n",
    "    return written_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_payloads_for_all_days()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363f551",
   "metadata": {},
   "source": [
    "## Temp Build Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3ff311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rounding] loaded 31 currency rules from WebService/Currency.csv\n",
      "[run-dir] WebService/data\\2025-07-01\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-01\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-01\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-02\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-02\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-02\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-03\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-03\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-03\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-04\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-04\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-04\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-05\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-05\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-05\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-06\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-06\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-06\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-07\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-07\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-07\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-08\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-08\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-08\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-09\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-09\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-09\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-10\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-10\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-10\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-11\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-11\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-11\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-12\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-12\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-12\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-13\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-13\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-13\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-14\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-14\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-14\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-15\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-15\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-15\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-16\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-16\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-16\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-17\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-17\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-17\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-18\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-18\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-18\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-19\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-19\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-19\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-20\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-20\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-20\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-21\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-21\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-21\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-22\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-22\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-22\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-23\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-23\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-23\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-24\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-24\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-24\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-25\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-25\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-25\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-26\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-26\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-26\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-27\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-27\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-27\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-28\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-28\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-28\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-29\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-29\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-29\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-30\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-30\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-30\\exchange_rates_payload.json\n",
      "[run-dir] WebService/data\\2025-07-31\n",
      "[exclude] dropped 12 P-pair rows by rule\n",
      "[rounding] zero-exceptions saved: 14 → WebService/data\\2025-07-31\\zero_exception.json\n",
      "[source] CSV(filtered), rows=386; skips={'csv_skipped': 0, 'bad_json': 0, 'unknown_shape': 0, 'json_row_skipped': 0, 'non_dict_row': 0}\n",
      "[written] WebService/data\\2025-07-31\\exchange_rates_payload.json\n",
      "[summary] payloads written: 31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from decimal import Decimal, ROUND_HALF_UP, InvalidOperation\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "BASE_DIR = \"WebService/data\"\n",
    "CURRENCY_RULES_CSV = \"WebService/Currency.csv\"  # round_digits,target_currency\n",
    "RUN_DIR_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "FNAME_DATE_RE = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})\\.json$\", re.IGNORECASE)\n",
    "\n",
    "def list_all_day_dirs(base_dir: str) -> List[str]:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        p = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(p) and RUN_DIR_PATTERN.match(name):\n",
    "            out.append(p)\n",
    "    out.sort()\n",
    "    return out\n",
    "\n",
    "def _norm_ccy(x: Any) -> str:\n",
    "    s = (x or \"\").strip().upper()\n",
    "    return s if re.fullmatch(r\"[A-Z]{3,4}\", s) else \"\"\n",
    "\n",
    "def _parse_rate(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    sx = str(x).strip().replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(sx)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _is_mmddyyyy(s: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{2}/\\d{2}/\\d{4}\", s or \"\"))\n",
    "\n",
    "def _to_ddmmyyyy_from_iso(iso: str) -> str:\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", iso or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _mmddyyyy_to_ddmmyyyy_dots(s: str) -> str:\n",
    "    m = re.fullmatch(r\"(\\d{2})/(\\d{2})/(\\d{4})\", s or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    mm, dd, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _key_ci(d: Dict[str, Any], *cands: str) -> Optional[str]:\n",
    "    lm = {k.lower(): k for k in d.keys()}\n",
    "    for c in cands:\n",
    "        k = lm.get(c.lower())\n",
    "        if k is not None:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def load_rounding_rules(csv_path: str) -> Dict[str, int]:\n",
    "    rules: Dict[str, int] = {}\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[warn] rounding rules CSV not found: {csv_path} (rates will not be rounded)\")\n",
    "        return rules\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            if not isinstance(row, dict):\n",
    "                continue\n",
    "            lower_map = {k.lower(): k for k in row.keys()}\n",
    "            k_digits = lower_map.get(\"round_digits\")\n",
    "            k_ccy    = lower_map.get(\"target_currency\")\n",
    "            if not k_digits or not k_ccy:\n",
    "                continue\n",
    "            try:\n",
    "                digits = int(str(row[k_digits]).strip())\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "            ccy = _norm_ccy(row[k_ccy])\n",
    "            if not ccy:\n",
    "                continue\n",
    "            if digits < 0: digits = 0\n",
    "            if digits > 12: digits = 12\n",
    "            rules[ccy] = digits\n",
    "    return rules\n",
    "\n",
    "def _round_rate_half_up(rate: float, digits: int) -> float:\n",
    "    try:\n",
    "        q = Decimal(\"1\").scaleb(-digits)\n",
    "        d = Decimal(str(rate)).quantize(q, rounding=ROUND_HALF_UP)\n",
    "        return float(d)\n",
    "    except (InvalidOperation, ValueError):\n",
    "        return rate\n",
    "\n",
    "def apply_rounding(rows: List[Dict[str, Any]], rules: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Apply rounding per rules to rows *in-place*.\n",
    "    NEW: If rounding would turn a non-zero rate into 0.0, keep the original rate\n",
    "    and record the case in a zero-exception list (returned).\n",
    "    Returns:\n",
    "        zero_exceptions: list of dicts describing rows where rounding would yield zero.\n",
    "    \"\"\"\n",
    "    zero_exceptions: List[Dict[str, Any]] = []\n",
    "    if not rules:\n",
    "        return zero_exceptions\n",
    "\n",
    "    for r in rows:\n",
    "        try:\n",
    "            to_ccy = _norm_ccy(r.get(\"ToCurrency\"))\n",
    "            if not to_ccy:\n",
    "                continue\n",
    "            digits = rules.get(to_ccy)\n",
    "            if digits is None:\n",
    "                continue\n",
    "            rate = r.get(\"ExchangeRate\")\n",
    "            if rate is None:\n",
    "                continue\n",
    "            rate_f = _parse_rate(rate)\n",
    "            if rate_f is None:\n",
    "                continue\n",
    "\n",
    "            rounded = _round_rate_half_up(rate_f, digits)\n",
    "\n",
    "            # If rounding makes a non-zero rate become zero, DON'T round.\n",
    "            if rate_f != 0.0 and rounded == 0.0:\n",
    "                zero_exceptions.append({\n",
    "                    \"FromCurrency\": r.get(\"FromCurrency\"),\n",
    "                    \"ToCurrency\": r.get(\"ToCurrency\"),\n",
    "                    \"ValidFrom\": r.get(\"ValidFrom\"),\n",
    "                    \"ExchangeRateType\": r.get(\"ExchangeRateType\"),\n",
    "                    \"Quotation\": r.get(\"Quotation\"),\n",
    "                    \"original_exchange_rate\": rate_f,\n",
    "                    \"attempted_digits\": digits,\n",
    "                    \"would_round_to\": rounded\n",
    "                })\n",
    "                # keep original: do NOT assign rounded\n",
    "                continue\n",
    "\n",
    "            # Normal case: apply rounded value\n",
    "            r[\"ExchangeRate\"] = rounded\n",
    "\n",
    "        except Exception:\n",
    "            # Swallow per-row errors to avoid breaking the whole day; no logging change here.\n",
    "            continue\n",
    "\n",
    "    return zero_exceptions\n",
    "\n",
    "def normalize_row_from_csv(raw: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    k_date_csv = _key_ci(raw, \"exChangeRateDate\")\n",
    "    valid_from = \"\"\n",
    "    if k_date_csv:\n",
    "        cand = (raw.get(k_date_csv) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    if not valid_from:\n",
    "        k_iso = _key_ci(raw, \"_service_date\")\n",
    "        if k_iso and raw.get(k_iso):\n",
    "            valid_from = _to_ddmmyyyy_from_iso(str(raw[k_iso]).strip())\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": \"M\",\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,\n",
    "        \"Quotation\": \"Direct\",\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "def normalize_row_from_json(raw: Dict[str, Any], fallback_iso_date: str = \"\") -> Optional[Dict[str, Any]]:\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    k_mmdd = _key_ci(raw, \"exChangeRateDate\")\n",
    "    k_other= _key_ci(raw, \"ValidFrom\", \"Date\", \"RateDate\", \"_service_date\")\n",
    "    valid_from = \"\"\n",
    "    if k_mmdd:\n",
    "        cand = (raw.get(k_mmdd) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    if not valid_from and k_other and raw.get(k_other):\n",
    "        s = str(raw[k_other]).strip()\n",
    "        if _is_mmddyyyy(s):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(s)\n",
    "        else:\n",
    "            valid_from = _to_ddmmyyyy_from_iso(s)\n",
    "\n",
    "    if not valid_from and fallback_iso_date:\n",
    "        valid_from = _to_ddmmyyyy_from_iso(fallback_iso_date)\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    k_type = _key_ci(raw, \"ExchangeRateType\", \"Type\")\n",
    "    k_quo  = _key_ci(raw, \"Quotation\", \"QuoteType\", \"Side\")\n",
    "    etype = str(raw.get(k_type)).strip() if (k_type and raw.get(k_type)) else \"M\"\n",
    "    quo   = str(raw.get(k_quo)).strip()  if (k_quo  and raw.get(k_quo))  else \"Direct\"\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": etype,\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,\n",
    "        \"Quotation\": quo,\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "def load_from_filtered_csv(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    path = os.path.join(run_dir, \"exchange_rates_agg.filtered.csv\")\n",
    "    if not os.path.isfile(path):\n",
    "        return []\n",
    "    with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        rows_raw = list(rdr)\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows_raw:\n",
    "        nr = normalize_row_from_csv(r)\n",
    "        if nr:\n",
    "            out.append(nr)\n",
    "        else:\n",
    "            skip_stats[\"csv_skipped\"] += 1\n",
    "    return out\n",
    "\n",
    "def load_from_filtered_jsons(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    base = os.path.join(run_dir, \"exchange_rates_json_filtered\")\n",
    "    if not os.path.isdir(base):\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for name in os.listdir(base):\n",
    "        if not name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        p = os.path.join(base, name)\n",
    "        fallback_iso = \"\"\n",
    "        m = FNAME_DATE_RE.match(name)\n",
    "        if m:\n",
    "            yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "            fallback_iso = f\"{yyyy}-{mm}-{dd}\"\n",
    "\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception:\n",
    "            skip_stats[\"bad_json\"] += 1\n",
    "            continue\n",
    "\n",
    "        if isinstance(obj, dict) and isinstance(obj.get(\"data\"), list):\n",
    "            iterable = obj[\"data\"]\n",
    "        elif isinstance(obj, list):\n",
    "            iterable = obj\n",
    "        elif isinstance(obj, dict):\n",
    "            iterable = [obj]\n",
    "        else:\n",
    "            skip_stats[\"unknown_shape\"] += 1\n",
    "            continue\n",
    "\n",
    "        for item in iterable:\n",
    "            if isinstance(item, dict):\n",
    "                nr = normalize_row_from_json(item, fallback_iso_date=fallback_iso)\n",
    "                if nr:\n",
    "                    out.append(nr)\n",
    "                else:\n",
    "                    skip_stats[\"json_row_skipped\"] += 1\n",
    "            else:\n",
    "                skip_stats[\"non_dict_row\"] += 1\n",
    "    return out\n",
    "\n",
    "def dedupe_sort(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    seen: Dict[Tuple[str, str, str, str, str], Dict[str, Any]] = {}\n",
    "    for r in rows:\n",
    "        key = (r[\"FromCurrency\"], r[\"ToCurrency\"], r[\"ValidFrom\"], r[\"ExchangeRateType\"], r[\"Quotation\"])\n",
    "        seen[key] = r\n",
    "    uniq = list(seen.values())\n",
    "\n",
    "    def _key(rw: Dict[str, Any]) -> Tuple[str, str, str]:\n",
    "        m = re.fullmatch(r\"(\\d{2})\\.(\\d{2})\\.(\\d{4})\", rw[\"ValidFrom\"])\n",
    "        ymd = f\"{m.group(3)}{m.group(2)}{m.group(1)}\" if m else \"00000000\"\n",
    "        return (ymd, rw[\"FromCurrency\"], rw[\"ToCurrency\"])\n",
    "\n",
    "    uniq.sort(key=_key)\n",
    "    return uniq\n",
    "\n",
    "# --- NEW: duplicate each logical row into M and P variants (before rounding) ---\n",
    "def _duplicate_rows_M_and_P(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        base = {\n",
    "            \"FromCurrency\": r.get(\"FromCurrency\"),\n",
    "            \"ToCurrency\": r.get(\"ToCurrency\"),\n",
    "            \"ValidFrom\": r.get(\"ValidFrom\"),\n",
    "            \"Quotation\": r.get(\"Quotation\"),\n",
    "            \"ExchangeRate\": r.get(\"ExchangeRate\"),\n",
    "        }\n",
    "        m_obj = dict(base)\n",
    "        m_obj[\"ExchangeRateType\"] = \"M\"\n",
    "        p_obj = dict(base)\n",
    "        p_obj[\"ExchangeRateType\"] = \"P\"\n",
    "        out.append(m_obj)\n",
    "        out.append(p_obj)\n",
    "    return out\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# --- NEW: global exclusions (applied across ALL days) --------------------------\n",
    "# Drop these PAIRS for ExchangeRateType == \"P\" regardless of date/rate.\n",
    "EXCLUDED_P_PAIRS: set[Tuple[str, str, str]] = {\n",
    "    (\"P\", \"SAR\", \"IQD\"),\n",
    "    (\"P\", \"JOD\", \"CHF\"),\n",
    "    (\"P\", \"JOD\", \"GBP\"),\n",
    "    (\"P\", \"JOD\", \"JPY\"),\n",
    "    (\"P\", \"IQD\", \"AED\"),\n",
    "    (\"P\", \"IQD\", \"EUR\"),\n",
    "    (\"P\", \"IQD\", \"JOD\"),\n",
    "    (\"P\", \"IQD\", \"SAR\"),\n",
    "    (\"P\", \"GBP\", \"JOD\"),\n",
    "    (\"P\", \"AED\", \"JOD\"),\n",
    "    (\"P\", \"AED\", \"IQD\"),\n",
    "    (\"P\", \"IQD\", \"USD\"),\n",
    "}\n",
    "\n",
    "def _filter_excluded_pairs(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    keep: List[Dict[str, Any]] = []\n",
    "    dropped = 0\n",
    "    for r in rows:\n",
    "        et = str(r.get(\"ExchangeRateType\", \"\")).strip().upper()\n",
    "        fc = _norm_ccy(r.get(\"FromCurrency\"))\n",
    "        tc = _norm_ccy(r.get(\"ToCurrency\"))\n",
    "        if (et, fc, tc) in EXCLUDED_P_PAIRS:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        keep.append(r)\n",
    "    if dropped:\n",
    "        print(f\"[exclude] dropped {dropped} P-pair rows by rule\")\n",
    "    else:\n",
    "        print(f\"[exclude] dropped 0 rows\")\n",
    "    return keep\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def build_payloads_for_all_days() -> List[str]:\n",
    "    written_paths: List[str] = []\n",
    "    day_dirs = list_all_day_dirs(BASE_DIR)\n",
    "    if not day_dirs:\n",
    "        print(f\"No day directories under {BASE_DIR} matching YYYY-MM-DD\")\n",
    "        return written_paths\n",
    "\n",
    "    rounding_rules = load_rounding_rules(CURRENCY_RULES_CSV)\n",
    "    if rounding_rules:\n",
    "        print(f\"[rounding] loaded {len(rounding_rules)} currency rules from {CURRENCY_RULES_CSV}\")\n",
    "    else:\n",
    "        print(f\"[rounding] no rules found (rates will not be rounded)\")\n",
    "\n",
    "    for run_dir in day_dirs:\n",
    "        print(f\"[run-dir] {run_dir}\")\n",
    "        skip_stats = {\n",
    "            \"csv_skipped\": 0,\n",
    "            \"bad_json\": 0,\n",
    "            \"unknown_shape\": 0,\n",
    "            \"json_row_skipped\": 0,\n",
    "            \"non_dict_row\": 0,\n",
    "        }\n",
    "\n",
    "        rows = load_from_filtered_csv(run_dir, skip_stats)\n",
    "        source = \"CSV(filtered)\"\n",
    "        if not rows:\n",
    "            rows = load_from_filtered_jsons(run_dir, skip_stats)\n",
    "            source = \"JSON(filtered)\"\n",
    "\n",
    "        # ✱ Fail fast if neither filtered source is present.\n",
    "        if not rows:\n",
    "            raise SystemExit(\n",
    "                f\"[FATAL] No filtered inputs found in {run_dir}.\\n\"\n",
    "                f\"Expected one of:\\n\"\n",
    "                f\"  - {os.path.join(run_dir, 'exchange_rates_agg.filtered.csv')}\\n\"\n",
    "                f\"  - {os.path.join(run_dir, 'exchange_rates_json_filtered')}/<*.json>\\n\"\n",
    "                f\"Run filter_to_needed_pairs.py AFTER fetch and BEFORE build.\"\n",
    "            )\n",
    "\n",
    "        # Duplicate every logical row into both M and P BEFORE rounding\n",
    "        rows = _duplicate_rows_M_and_P(rows)\n",
    "\n",
    "        # NEW: drop the specified P pairs for ALL days before rounding/zero-exception logic\n",
    "        rows = _filter_excluded_pairs(rows)\n",
    "\n",
    "        # Apply rounding with zero-exception safeguard (covers both M and P)\n",
    "        zero_exceptions = apply_rounding(rows, rounding_rules)\n",
    "\n",
    "        # If any rounding would have produced zero, persist them to zero_exception.json\n",
    "        if zero_exceptions:\n",
    "            zp = os.path.join(run_dir, \"zero_exception.json\")\n",
    "            try:\n",
    "                with open(zp, \"w\", encoding=\"utf-8\") as zf:\n",
    "                    json.dump(zero_exceptions, zf, ensure_ascii=False, indent=2)\n",
    "                print(f\"[rounding] zero-exceptions saved: {len(zero_exceptions)} → {zp}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"[warn] failed to write zero_exception.json in {run_dir}: {ex}\")\n",
    "        else:\n",
    "            print(f\"[rounding] zero-exceptions: 0\")\n",
    "\n",
    "        rows = dedupe_sort(rows)\n",
    "        print(f\"[source] {source}, rows={len(rows)}; skips={skip_stats}\")\n",
    "\n",
    "        out_path = os.path.join(run_dir, \"exchange_rates_payload.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[written] {out_path}\")\n",
    "        written_paths.append(out_path)\n",
    "\n",
    "    print(f\"[summary] payloads written: {len(written_paths)}\")\n",
    "    return written_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_payloads_for_all_days()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c106b1",
   "metadata": {},
   "source": [
    "# Fixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff064a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_fixer_range.py\n",
    "# pip install requests pandas\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "from typing import Iterable, Dict, Any, List, Optional, Tuple, Set\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import ConnectTimeout, ReadTimeout, ProxyError, SSLError, ConnectionError as ReqConnectionError\n",
    "\n",
    "# === CONFIG ===\n",
    "ACCESS_KEY = \"4e24104d947d9a92ecad3d7c44059f9f\"\n",
    "FIXER_BASE_URL = \"https://data.fixer.io/api\"  # will call /YYYY-MM-DD\n",
    "DATE_START = date(2025, 7, 16)\n",
    "DATE_END   = date(2025, 7, 31)\n",
    "\n",
    "NEEDED_FILE = os.path.join(\"WebService\", \"needed_rates.txt\")  # lines like: EUR/JOD\n",
    "BASE_SAVE_DIR = \"WebService\"\n",
    "\n",
    "# Timeouts / retries (HTTP)\n",
    "CONNECT_TIMEOUT = 5\n",
    "READ_TIMEOUT = 30\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF_BASE = 2\n",
    "\n",
    "DATE_RE_MMDDYYYY = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "\n",
    "# === Utilities ===\n",
    "def mmddyyyy(d: date) -> str:\n",
    "    return f\"{d.month:02d}/{d.day:02d}/{d.year:04d}\"\n",
    "\n",
    "def daterange_days(start: date, end: date) -> Iterable[date]:\n",
    "    d = start\n",
    "    while d <= end:\n",
    "        yield d\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"Accept\": \"application/json\"})\n",
    "    s.trust_env = False\n",
    "    s.proxies.update({\"http\": None, \"https\": None})\n",
    "    return s\n",
    "\n",
    "def _fetch_fixer_day(d: date, symbols: List[str], session: requests.Session) -> Dict[str, Any]:\n",
    "    url = f\"{FIXER_BASE_URL}/{d.isoformat()}\"\n",
    "    params = {\"access_key\": ACCESS_KEY, \"symbols\": \",\".join(sorted(set(symbols)))}\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            if not data.get(\"success\", False):\n",
    "                raise RuntimeError(f\"Fixer error: {data.get('error', data)}\")\n",
    "            return data\n",
    "        except (ConnectTimeout, ReadTimeout, ProxyError, SSLError, ReqConnectionError, requests.HTTPError) as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(RETRY_BACKOFF_BASE ** attempt, 8))\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(RETRY_BACKOFF_BASE ** attempt, 8))\n",
    "    raise RuntimeError(f\"[{d}] Failed after {MAX_RETRIES} attempts: {last_err}\")\n",
    "\n",
    "def _read_needed_pairs(path: str) -> List[Tuple[str, str]]:\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"needed_rates file not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            if \"/\" not in s:\n",
    "                parts = s.replace(\" \", \"/\").split(\"/\")\n",
    "            else:\n",
    "                parts = s.split(\"/\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            a = parts[0].strip().upper()\n",
    "            b = parts[1].strip().upper()\n",
    "            if re.fullmatch(r\"[A-Z]{3,4}\", a) and re.fullmatch(r\"[A-Z]{3,4}\", b):\n",
    "                pairs.append((a, b))\n",
    "    if not pairs:\n",
    "        raise ValueError(\"No valid pairs loaded from needed_rates.txt\")\n",
    "    return pairs\n",
    "\n",
    "def _symbols_from_pairs(pairs: List[Tuple[str, str]]) -> List[str]:\n",
    "    s: Set[str] = set()\n",
    "    for a, b in pairs:\n",
    "        s.add(a); s.add(b)\n",
    "    # Free Fixer base is EUR; include EUR to be safe in math\n",
    "    s.add(\"EUR\")\n",
    "    return sorted(s)\n",
    "\n",
    "def _compute_pair_rate(a: str, b: str, rates_vs_eur: Dict[str, float]) -> Optional[float]:\n",
    "    # A->B = (EUR->B)/(EUR->A)\n",
    "    ra = rates_vs_eur.get(a)\n",
    "    rb = rates_vs_eur.get(b)\n",
    "    if ra is None or rb is None or ra == 0:\n",
    "        return None\n",
    "    return rb / ra\n",
    "\n",
    "def _save_day_json(out_dir: str, d: date, rows: List[Dict[str, Any]]) -> str:\n",
    "    json_dir = os.path.join(out_dir, \"exchange_rates_json\")\n",
    "    ensure_dir(json_dir)\n",
    "    p = os.path.join(json_dir, f\"{d.isoformat()}.json\")\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"data\": rows}, f, ensure_ascii=False, indent=2)\n",
    "    return p\n",
    "\n",
    "def _append_csv(out_dir: str, rows: List[Dict[str, Any]]) -> None:\n",
    "    csv_path = os.path.join(out_dir, \"exchange_rates_agg.csv\")\n",
    "    if not rows:\n",
    "        return\n",
    "    # discover headers\n",
    "    all_keys = set()\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rdr = csv.DictReader(f)\n",
    "            all_keys.update(rdr.fieldnames or [])\n",
    "    for r in rows:\n",
    "        all_keys.update(r.keys())\n",
    "    fieldnames = sorted(all_keys)\n",
    "\n",
    "    # load existing\n",
    "    existing: List[Dict[str, Any]] = []\n",
    "    if os.path.exists(csv_path):\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rdr = csv.DictReader(f)\n",
    "            for r in rdr:\n",
    "                existing.append(r)\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in existing:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "def main():\n",
    "    pairs = _read_needed_pairs(NEEDED_FILE)\n",
    "    symbols = _symbols_from_pairs(pairs)\n",
    "    session = _make_session()\n",
    "\n",
    "    for d in daterange_days(DATE_START, DATE_END):\n",
    "        day_dir = os.path.join(BASE_SAVE_DIR, d.isoformat())\n",
    "        ensure_dir(day_dir)\n",
    "\n",
    "        # fetch once per day with all needed symbols\n",
    "        data = _fetch_fixer_day(d, symbols, session)\n",
    "        rates = dict(data.get(\"rates\", {}))\n",
    "        rates[\"EUR\"] = 1.0  # guarantee\n",
    "\n",
    "        # compute only requested pairs; output unified row shape your later steps expect\n",
    "        mmdd = mmddyyyy(d)\n",
    "        day_rows: List[Dict[str, Any]] = []\n",
    "        skipped: List[Tuple[str, str]] = []\n",
    "\n",
    "        for a, b in pairs:\n",
    "            r = _compute_pair_rate(a, b, rates)\n",
    "            if r is None:\n",
    "                skipped.append((a, b))\n",
    "                continue\n",
    "            row = {\n",
    "                \"ExchangeRateType\": \"M\",\n",
    "                \"FromCurrency\": a,\n",
    "                \"ToCurrency\": b,\n",
    "                \"exChangeRateDate\": mmdd,      # MM/DD/YYYY\n",
    "                \"Quotation\": \"Direct\",\n",
    "                \"exChangeRateValue\": round(float(r), 9),\n",
    "                \"_source\": \"Fixer\",\n",
    "                \"_fixer_date\": data.get(\"date\", d.isoformat()),\n",
    "                \"_base\": \"EUR\"\n",
    "            }\n",
    "            day_rows.append(row)\n",
    "\n",
    "        # save JSON + CSV inside the day folder\n",
    "        _save_day_json(day_dir, d, day_rows)\n",
    "        _append_csv(day_dir, day_rows)\n",
    "\n",
    "        # simple log\n",
    "        print(f\"[{d}] kept={len(day_rows)} skipped={len(skipped)} → {day_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_to_needed_pairs.py\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from typing import Dict, Any, List, Tuple, Set\n",
    "\n",
    "BASE_DIR = \"WebService\"\n",
    "NEEDED_FILE = os.path.join(\"Weneeded_rates.txt\")  # AAA/BBB per line\n",
    "\n",
    "# match single-day directories: YYYY-MM-DD\n",
    "RUN_DIR_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _norm_currency(s: str) -> str:\n",
    "    return (s or \"\").strip().upper()\n",
    "\n",
    "def _find_key_case_insensitive(d: Dict[str, Any], *candidates: str) -> str:\n",
    "    lower_map = {k.lower(): k for k in d.keys()}\n",
    "    for c in candidates:\n",
    "        k = lower_map.get(c.lower())\n",
    "        if k is not None:\n",
    "            return k\n",
    "    return \"\"\n",
    "\n",
    "def _row_pair(row: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    if not isinstance(row, dict):\n",
    "        return \"\", \"\"\n",
    "    from_k = _find_key_case_insensitive(row, \"FromCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    to_k   = _find_key_case_insensitive(row, \"ToCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    f = _norm_currency(row.get(from_k, \"\")) if from_k else \"\"\n",
    "    t = _norm_currency(row.get(to_k, \"\"))   if to_k else \"\"\n",
    "    return f, t\n",
    "\n",
    "def load_needed_pairs(path: str) -> Set[Tuple[str, str]]:\n",
    "    pairs: Set[Tuple[str, str]] = set()\n",
    "    if not os.path.isfile(path):\n",
    "        print(f\"[warn] needed list not found: {path} (no filtering will be applied)\")\n",
    "        return pairs\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):\n",
    "                continue\n",
    "            if \"/\" not in s:\n",
    "                parts = s.replace(\" \", \"/\").split(\"/\")\n",
    "            else:\n",
    "                parts = s.split(\"/\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            a, b = _norm_currency(parts[0]), _norm_currency(parts[1])\n",
    "            if a and b:\n",
    "                pairs.add((a, b))\n",
    "    return pairs\n",
    "\n",
    "def should_keep(row: Dict[str, Any], include_set: Set[Tuple[str, str]]) -> bool:\n",
    "    if not include_set:\n",
    "        return True  # if list missing, keep everything\n",
    "    f, t = _row_pair(row)\n",
    "    if not f or not t:\n",
    "        return False\n",
    "    return (f, t) in include_set\n",
    "\n",
    "def filter_json_object(obj: Any, include_set: Set[Tuple[str, str]]) -> Any:\n",
    "    try:\n",
    "        if isinstance(obj, list):\n",
    "            return [r for r in obj if not isinstance(r, dict) or should_keep(r, include_set)]\n",
    "        if isinstance(obj, dict):\n",
    "            if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "                new_obj = dict(obj)\n",
    "                new_obj[\"data\"] = [r for r in obj[\"data\"] if not isinstance(r, dict) or should_keep(r, include_set)]\n",
    "                return new_obj\n",
    "            # single object → keep only if it matches; else empty list-like stub\n",
    "            return obj if should_keep(obj, include_set) else {\"data\": []}\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return obj\n",
    "\n",
    "def filter_all_jsons(run_dir: str, include_set: Set[Tuple[str, str]]) -> Tuple[int, int]:\n",
    "    json_in = os.path.join(run_dir, \"exchange_rates_json\")\n",
    "    json_out = os.path.join(run_dir, \"exchange_rates_json_filtered\")\n",
    "    ensure_dir(json_out)\n",
    "\n",
    "    processed = 0\n",
    "    written = 0\n",
    "\n",
    "    if not os.path.isdir(json_in):\n",
    "        print(f\"[warn] JSON input dir not found: {json_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    for name in os.listdir(json_in):\n",
    "        if not name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        inp = os.path.join(json_in, name)\n",
    "        out = os.path.join(json_out, name)\n",
    "        try:\n",
    "            with open(inp, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "            new_obj = filter_json_object(obj, include_set)\n",
    "            with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(new_obj, f, ensure_ascii=False, indent=2)\n",
    "            processed += 1\n",
    "            written += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[json] skip {inp}: {e}\")\n",
    "            processed += 1\n",
    "    return processed, written\n",
    "\n",
    "def filter_csv(run_dir: str, include_set: Set[Tuple[str, str]]) -> Tuple[int, int]:\n",
    "    csv_in = os.path.join(run_dir, \"exchange_rates_agg.csv\")\n",
    "    csv_out = os.path.join(run_dir, \"exchange_rates_agg.filtered.csv\")\n",
    "\n",
    "    if not os.path.isfile(csv_in):\n",
    "        print(f\"[warn] CSV input not found: {csv_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    with open(csv_in, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        fieldnames = rdr.fieldnames or []\n",
    "        rows = list(rdr)\n",
    "\n",
    "    if not fieldnames:\n",
    "        print(f\"[warn] CSV has no header: {csv_in}\")\n",
    "        return (0, 0)\n",
    "\n",
    "    kept: List[Dict[str, Any]] = []\n",
    "    for r in rows:\n",
    "        if should_keep(r, include_set):\n",
    "            kept.append(r)\n",
    "\n",
    "    # Ensure FromCurrency/ToCurrency exist in header\n",
    "    if \"FromCurrency\" not in fieldnames:\n",
    "        fieldnames = fieldnames + [\"FromCurrency\"]\n",
    "    if \"ToCurrency\" not in fieldnames:\n",
    "        fieldnames = fieldnames + [\"ToCurrency\"]\n",
    "\n",
    "    with open(csv_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in kept:\n",
    "            out_row = {k: r.get(k, \"\") for k in fieldnames}\n",
    "            fcur, tcur = _row_pair(r)\n",
    "            if fcur and not out_row.get(\"FromCurrency\"):\n",
    "                out_row[\"FromCurrency\"] = fcur\n",
    "            if tcur and not out_row.get(\"ToCurrency\"):\n",
    "                out_row[\"ToCurrency\"] = tcur\n",
    "            w.writerow(out_row)\n",
    "\n",
    "    return (len(rows), len(kept))\n",
    "\n",
    "def list_all_day_dirs(base_dir: str) -> List[str]:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        p = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(p) and RUN_DIR_PATTERN.match(name):\n",
    "            out.append(p)\n",
    "    out.sort()  # chronological by name\n",
    "    return out\n",
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    day_dirs = list_all_day_dirs(BASE_DIR)\n",
    "    if not day_dirs:\n",
    "        raise SystemExit(f\"No day directories found under {BASE_DIR} matching YYYY-MM-DD\")\n",
    "\n",
    "    needed = load_needed_pairs(NEEDED_FILE)\n",
    "    total_json_in = total_json_out = total_csv_in = total_csv_out = 0\n",
    "\n",
    "    for run_dir in day_dirs:\n",
    "        print(f\"\\n[run-dir] {run_dir}\")\n",
    "        j_in, j_out = filter_all_jsons(run_dir, needed)\n",
    "        print(f\"[done] JSON files processed={j_in}, written={j_out} → {os.path.join(run_dir, 'exchange_rates_json_filtered')}\")\n",
    "        c_in, c_out = filter_csv(run_dir, needed)\n",
    "        print(f\"[done] CSV rows in={c_in}, out={c_out} → {os.path.join(run_dir, 'exchange_rates_agg.filtered.csv')}\")\n",
    "        total_json_in += j_in; total_json_out += j_out\n",
    "        total_csv_in  += c_in; total_csv_out  += c_out\n",
    "\n",
    "    print(f\"\\n[summary] JSON in={total_json_in}, JSON out={total_json_out}; CSV in={total_csv_in}, CSV out={total_csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_payload.py\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "\n",
    "BASE_DIR = \"WebService\"\n",
    "RUN_DIR_PATTERN = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "FNAME_DATE_RE = re.compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})\\.json$\", re.IGNORECASE)\n",
    "\n",
    "# ---------- utils ----------\n",
    "def list_all_day_dirs(base_dir: str) -> List[str]:\n",
    "    if not os.path.isdir(base_dir):\n",
    "        return []\n",
    "    out: List[str] = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        p = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(p) and RUN_DIR_PATTERN.match(name):\n",
    "            out.append(p)\n",
    "    out.sort()  # chronological\n",
    "    return out\n",
    "\n",
    "def _norm_ccy(x: Any) -> str:\n",
    "    s = (x or \"\").strip().upper()\n",
    "    return s if re.fullmatch(r\"[A-Z]{3,4}\", s) else \"\"\n",
    "\n",
    "def _parse_rate(x: Any) -> Optional[float]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    sx = str(x).strip().replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(sx)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _is_mmddyyyy(s: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{2}/\\d{2}/\\d{4}\", s or \"\"))\n",
    "\n",
    "def _to_ddmmyyyy_from_iso(iso: str) -> str:\n",
    "    # \"YYYY-MM-DD\" -> \"DD.MM.YYYY\"\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", iso or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _mmddyyyy_to_ddmmyyyy_dots(s: str) -> str:\n",
    "    # \"MM/DD/YYYY\" -> \"DD.MM.YYYY\"\n",
    "    m = re.fullmatch(r\"(\\d{2})/(\\d{2})/(\\d{4})\", s or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    mm, dd, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "    return f\"{dd}.{mm}.{yyyy}\"\n",
    "\n",
    "def _key_ci(d: Dict[str, Any], *cands: str) -> Optional[str]:\n",
    "    lm = {k.lower(): k for k in d.keys()}\n",
    "    for c in cands:\n",
    "        k = lm.get(c.lower())\n",
    "        if k is not None:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "# ---------- row normalization ----------\n",
    "def normalize_row_from_csv(raw: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Map to:\n",
    "    {\n",
    "      \"ExchangeRateType\": \"M\",\n",
    "      \"FromCurrency\": \"...\",\n",
    "      \"ToCurrency\": \"...\",\n",
    "      \"ValidFrom\": \"DD.MM.YYYY\",\n",
    "      \"Quotation\": \"Direct\",\n",
    "      \"ExchangeRate\": float\n",
    "    }\n",
    "    \"\"\"\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    # Prefer exChangeRateDate if present (MM/DD/YYYY), else _service_date (YYYY-MM-DD)\n",
    "    k_date_csv = _key_ci(raw, \"exChangeRateDate\")\n",
    "    valid_from = \"\"\n",
    "    if k_date_csv:\n",
    "        cand = (raw.get(k_date_csv) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    if not valid_from:\n",
    "        k_iso = _key_ci(raw, \"_service_date\")  # YYYY-MM-DD\n",
    "        if k_iso and raw.get(k_iso):\n",
    "            valid_from = _to_ddmmyyyy_from_iso(str(raw[k_iso]).strip())\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": \"M\",\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,  # DD.MM.YYYY\n",
    "        \"Quotation\": \"Direct\",\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "def normalize_row_from_json(raw: Dict[str, Any], fallback_iso_date: str = \"\") -> Optional[Dict[str, Any]]:\n",
    "    k_from = _key_ci(raw, \"FromCurrency\", \"sourceCurrency\", \"From\", \"BaseCurrency\", \"SourceCurrency\")\n",
    "    k_to   = _key_ci(raw, \"ToCurrency\", \"targetCurrency\", \"To\", \"QuoteCurrency\", \"TargetCurrency\")\n",
    "    fcur = _norm_ccy(raw.get(k_from, \"\")) if k_from else \"\"\n",
    "    tcur = _norm_ccy(raw.get(k_to, \"\"))   if k_to else \"\"\n",
    "    if not fcur or not tcur:\n",
    "        return None\n",
    "\n",
    "    k_rate = _key_ci(raw, \"exChangeRateValue\", \"ExchangeRate\", \"Rate\", \"FxRate\", \"Value\")\n",
    "    rate = _parse_rate(raw.get(k_rate)) if k_rate else None\n",
    "    if rate is None:\n",
    "        return None\n",
    "\n",
    "    # Try MM/DD/YYYY field first\n",
    "    k_mmdd = _key_ci(raw, \"exChangeRateDate\")\n",
    "    k_other= _key_ci(raw, \"ValidFrom\", \"Date\", \"RateDate\", \"_service_date\")\n",
    "    valid_from = \"\"\n",
    "    if k_mmdd:\n",
    "        cand = (raw.get(k_mmdd) or \"\").strip()\n",
    "        if _is_mmddyyyy(cand):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(cand)\n",
    "\n",
    "    # Then try other date fields (may be MM/DD/YYYY or ISO)\n",
    "    if not valid_from and k_other and raw.get(k_other):\n",
    "        s = str(raw[k_other]).strip()\n",
    "        if _is_mmddyyyy(s):\n",
    "            valid_from = _mmddyyyy_to_ddmmyyyy_dots(s)\n",
    "        else:\n",
    "            valid_from = _to_ddmmyyyy_from_iso(s)\n",
    "\n",
    "    # Fallback to filename ISO date\n",
    "    if not valid_from and fallback_iso_date:\n",
    "        valid_from = _to_ddmmyyyy_from_iso(fallback_iso_date)\n",
    "\n",
    "    if not valid_from:\n",
    "        return None\n",
    "\n",
    "    k_type = _key_ci(raw, \"ExchangeRateType\", \"Type\")\n",
    "    k_quo  = _key_ci(raw, \"Quotation\", \"QuoteType\", \"Side\")\n",
    "    etype = str(raw.get(k_type)).strip() if (k_type and raw.get(k_type)) else \"M\"\n",
    "    quo   = str(raw.get(k_quo)).strip()  if (k_quo  and raw.get(k_quo))  else \"Direct\"\n",
    "\n",
    "    return {\n",
    "        \"ExchangeRateType\": etype,\n",
    "        \"FromCurrency\": fcur,\n",
    "        \"ToCurrency\": tcur,\n",
    "        \"ValidFrom\": valid_from,  # DD.MM.YYYY\n",
    "        \"Quotation\": quo,\n",
    "        \"ExchangeRate\": rate,\n",
    "    }\n",
    "\n",
    "# ---------- loaders ----------\n",
    "def load_from_filtered_csv(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    path = os.path.join(run_dir, \"exchange_rates_agg.filtered.csv\")\n",
    "    if not os.path.isfile(path):\n",
    "        return []\n",
    "    with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        rows_raw = list(rdr)\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for r in rows_raw:\n",
    "        nr = normalize_row_from_csv(r)\n",
    "        if nr:\n",
    "            out.append(nr)\n",
    "        else:\n",
    "            skip_stats[\"csv_skipped\"] += 1\n",
    "    return out\n",
    "\n",
    "def load_from_filtered_jsons(run_dir: str, skip_stats: Dict[str, int]) -> List[Dict[str, Any]]:\n",
    "    base = os.path.join(run_dir, \"exchange_rates_json_filtered\")\n",
    "    if not os.path.isdir(base):\n",
    "        return []\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for name in os.listdir(base):\n",
    "        if not name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        p = os.path.join(base, name)\n",
    "        fallback_iso = \"\"\n",
    "        m = FNAME_DATE_RE.match(name)\n",
    "        if m:\n",
    "            yyyy, mm, dd = m.group(1), m.group(2), m.group(3)\n",
    "            fallback_iso = f\"{yyyy}-{mm}-{dd}\"\n",
    "\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception:\n",
    "            skip_stats[\"bad_json\"] += 1\n",
    "            continue\n",
    "\n",
    "        if isinstance(obj, dict) and isinstance(obj.get(\"data\"), list):\n",
    "            iterable = obj[\"data\"]\n",
    "        elif isinstance(obj, list):\n",
    "            iterable = obj\n",
    "        elif isinstance(obj, dict):\n",
    "            iterable = [obj]\n",
    "        else:\n",
    "            skip_stats[\"unknown_shape\"] += 1\n",
    "            continue\n",
    "\n",
    "        for item in iterable:\n",
    "            if isinstance(item, dict):\n",
    "                nr = normalize_row_from_json(item, fallback_iso_date=fallback_iso)\n",
    "                if nr:\n",
    "                    out.append(nr)\n",
    "                else:\n",
    "                    skip_stats[\"json_row_skipped\"] += 1\n",
    "            else:\n",
    "                skip_stats[\"non_dict_row\"] += 1\n",
    "    return out\n",
    "\n",
    "def dedupe_sort(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    # Deduplicate by (From, To, ValidFrom, Type, Quotation); keep last seen\n",
    "    seen: Dict[Tuple[str, str, str, str, str], Dict[str, Any]] = {}\n",
    "    for r in rows:\n",
    "        key = (r[\"FromCurrency\"], r[\"ToCurrency\"], r[\"ValidFrom\"], r[\"ExchangeRateType\"], r[\"Quotation\"])\n",
    "        seen[key] = r\n",
    "    uniq = list(seen.values())\n",
    "\n",
    "    # Sort by \"DD.MM.YYYY\": convert to YYYYMMDD for sorting\n",
    "    def _key(r: Dict[str, Any]) -> Tuple[str, str, str]:\n",
    "        m = re.fullmatch(r\"(\\d{2})\\.(\\d{2})\\.(\\d{4})\", r[\"ValidFrom\"])\n",
    "        if m:\n",
    "            dd, mm, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "            ymd = f\"{yyyy}{mm}{dd}\"\n",
    "        else:\n",
    "            ymd = \"00000000\"\n",
    "        return (ymd, r[\"FromCurrency\"], r[\"ToCurrency\"])\n",
    "\n",
    "    uniq.sort(key=_key)\n",
    "    return uniq\n",
    "\n",
    "# ---------- main ----------\n",
    "def build_payloads_for_all_days() -> List[str]:\n",
    "    written_paths: List[str] = []\n",
    "    day_dirs = list_all_day_dirs(BASE_DIR)\n",
    "    if not day_dirs:\n",
    "        print(f\"No day directories under {BASE_DIR} matching YYYY-MM-DD\")\n",
    "        return written_paths\n",
    "\n",
    "    for run_dir in day_dirs:\n",
    "        print(f\"[run-dir] {run_dir}\")\n",
    "        skip_stats = {\n",
    "            \"csv_skipped\": 0,\n",
    "            \"bad_json\": 0,\n",
    "            \"unknown_shape\": 0,\n",
    "            \"json_row_skipped\": 0,\n",
    "            \"non_dict_row\": 0,\n",
    "        }\n",
    "\n",
    "        rows = load_from_filtered_csv(run_dir, skip_stats)\n",
    "        source = \"CSV\"\n",
    "        if not rows:\n",
    "            rows = load_from_filtered_jsons(run_dir, skip_stats)\n",
    "            source = \"JSON\"\n",
    "\n",
    "        rows = dedupe_sort(rows)\n",
    "        print(f\"[source] {source}, rows={len(rows)}; skips={skip_stats}\")\n",
    "\n",
    "        out_path = os.path.join(run_dir, \"exchange_rates_payload.json\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[written] {out_path}\")\n",
    "        written_paths.append(out_path)\n",
    "\n",
    "    print(f\"[summary] payloads written: {len(written_paths)}\")\n",
    "    return written_paths\n",
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    build_payloads_for_all_days()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
